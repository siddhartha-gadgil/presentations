<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>ATP in Maths</title>

	<meta name="description" content="ICTS 2024 summer school lecture">
	<meta name="author" content="Siddhartha Gadgil">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport"
		content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">
	<link rel="stylesheet" href="kwk.css">

	<link rel="icon" href="IIScLogo.jpg">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside this container is displayed as a slide -->
		<section class="slides">
			<section data-transition="slide">
				<h3> Automated Theorem Proving</h3>
				<h4>and its</h4>
				<h3>role in Mathematics</h3>
				<h4>Siddhartha Gadgil</h4>
				<p>Department of Mathematics</p>
				<p>Indian Institute of Science, Bangalore</p>


			</section>

			<section>
				<ul>
					<li> Automated Theorem Proving has many components:
						<ul>
							<li class="fragment"> Deep learning (AI/ML): both <em>Foundation models</em> and domain
								specific learning.
							</li>
							<li class="fragment"><em>Interactive Theorem Provers</em>, especially Lean and its
								Mathematical library.</li>
							<li class="fragment"> <em>Symbolic AI</em> (deduction engines), Symbolic Algebra and other
								mathematical software.</li>
						</ul>
					</li>
					<li class="fragment"> These components complement each other very well, mirroring different aspects
						of our reasoning.</strong>
					</li>

				</ul>
			</section>


			<section data-transition="zoom-in slide-out" data-background-image="campus.jpg">
				<h1>Mathematical Proofs</h1>
			</section>

			<section data-background-color="teal">
				<h4> A proof: Sum of Angles in a Triangle</h4>
				<ul>
					<li class="fragment"> Consider a triangle $ABC$.</li>
					<li class="fragment"> Draw a line through $A$ parallel to $BC$.
						<center><img src="angle_sum.png" height="250" /></center>
					</li>
					<li class="fragment"> Use equality of interior opposite angles.</li>
					<li class="fragment"> Use angles on a line add up to 180 degrees.</li>

				</ul>
			</section>
			<section data-background-color="teal">
				<h4>Another proof: infinitely many primes</h4>
				<ul>
					<li class="fragment"> For any $n$, we show that there exists a prime $p$ such that $p > n$.</li>
					<li class="fragment"> Consider the number $n!+1$, where $n! = 1 \times 2 \times \dots \times n$.
					</li>
					<li class="fragment"> Let $p$ be the smallest prime factor of $n!+1$.
					</li>
					<li class="fragment"> If $p \leq n$ then $p | n!$ so $p \nmid n!+1$.</li>
					<li class="fragment"> Hence $p$ is a prime greater than $n$.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">Components of Proving (Reasoning)</h4>
				<ul>
					<li class="fragment"><strong>Intuition:</strong> Analogies, Associations etc.
						<ul>
							<li class="fragment">
								Auxiliary Constructions.
							</li>
							<li class="fragment">
								<p>Theorems/Definitions to use.</p>
							</li>
							<li class="fragment">
								Intermediate Lemmas and statements.
							</li>
						</ul>
					</li>
					<li class="fragment">
						<p><strong>Logical/Algorithmic:</strong> Deductions, Computations.</p>
					</li>
					<li class="fragment">For mathematical theories, or complex proofs, we also need
						to decompose problems and manage complexity, i.e., <em>Modularity</em>.
					</li>
				</ul>

			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">Modularity/Abstraction</h4>
				<ul>
					<li class="fragment"><strong>Modularity:</strong> being decomposable into parts:
						<ul>
							<li class="fragment">
								Often in a hierarchical way.
							</li>
							<li class="fragment">
								So that the <em>interface (surface)</em> through which the component interacts is
								relatively simple.
							</li>
							<li class="fragment">
								Details of the component are <em>abstracted</em>.
							</li>
						</ul>
					</li>
					<li class="fragment">
						Knowledge is also modular, allowing focus on a specific part and level of abstraction.
					</li>
					<li class="fragment">
						In Mathematics, we use <strong>definitions</strong> and <strong>theorems</strong> for
						abstraction: <em>fully unfolded proofs</em> are incomprehensible.
					</li>
				</ul>

			</section>


			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">

				<h2 style="color:#fffdd0">Interactive Theorem Provers</h2>
			</section>



			<section data-transition="concave" data-background-color="#002b36">
				<ul>
					<li><em>Interactive Theorem Provers</em> are software systems where proofs are
						obtained by human-machine
						collaboration, with modular foundations.</li>
					<li class="fragment">The computer both finds (parts of) proofs and verifies
						<strong>correctness</strong>.
					</li>
					<li class="fragment">Lean ensures a super-human
						level of correctness, hopefully <strong>without super-human effort</strong>.
					</li>
					<li class="fragment">Formalization builds on existing formalization; Lean's <em>Mathlib</em> has most undergraduate
						mathematics in it, and some advanced topics.
					</li>
					<li class="fragment">We see an <a
							href="https://live.lean-lang.org/#codez=JYWwDg9gTgLgBAWQIYwBYBtgCMBQEwCmAdnAHIo44D0AtDXAKICuAxpgCYDkAznGgdAIg4EEvzgAqCcCIAzGcBhN2BEbLhgooAtykA6HAAkCUVQHNgAN2JwZfVKtnQQALjhOocAtagBPOAAGRAEANPYmqgQAHsDcMLxIGlogqkRMIFgmgWBwgKZEcMF6cDRUOPyCwtGx8QD6MvJEigQ1mtq8ABQkboCohACUcG6AwEQaYSSAJkQacIDkRGQoegAKyao5LgC8cFi+OHBw6ATwK+sgMgBiSCxwnXAAhHAA1HAAjL3bcKhI1u6PAwU3909wQAGRAC1hstjsdkgwGB0P5yDA9ERmhBZDUzPBXpDobC4NxWCwaugYDU8SwLpi4FCYf5ZOcYNBgEh0C0INxXu9PtCfvCFktJqDNhSqTjjkQzgTWikKdxQDkANpImootEwMKkglEkn4sK0lj0rRMlncAC67I+qRWvwmhzBQux/j2StRRAgxLMBApMhgUAgbwpHNUqEAgQQ/HKAYiJfrcBeCIVjqXB2JZ2DVdfrGeg4BS4yLTucjZnYztonS/bGA29AEEEobgEe+oPaPMTyaQ7GTwFkqK0ZlQ8GDvT0ACZLqLxTUm3Aai9Y8W9RowIjXWOk0qkZXXkxuMscEA"
							target="_blank"> example</a> on the lean server.

					</li>
				</ul>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>Who guards the guards?</h4>
				<ul>
					<li class="fragment">
						A computer verified proof is only as trustworthy as the system that verified the proof.
					</li>
					<li class="fragment">
						Following the <em>de Bruijn</em> principle, proofs are <em>verified</em> by a small
						<em>trusted
							kernel</em>, which can be thoroughly checked.
					</li>
					<li class="fragment">
						For example, the Lean theorem prover has independent (small) proof checkers, one written in
						a different language (Rust).
					</li>
					<li class="fragment">
						In a specific case, one only has to check that the <strong>definitions</strong> and the
						<strong>statement</strong> are correct, which is typically not hard especially if they use a
						library.
					</li>
				</ul>

			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>The Liquid Tensor experiment</h4>
				<ul>
					<li class="fragment">In December 2020, Peter Scholze challenged the formalization community to
						computer verify a foundational theorem of <em>Condensed
							Mathematics</em>.</li>
					<blockquote class="fragment">
						— with this theorem, the hope that the condensed formalism can be fruitfully applied to real
						functional analysis stands or falls. I think the theorem is of utmost foundational importance,
						so being 99.9% sure is not enough.
					</blockquote>

				</ul>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>This was taken up by the Lean community led by Johann Commelin, with active
						collaboration from Scholze.</li>

					<li class="fragment">
						In about six months a major step, <strong>Theorem 9.4</strong> of Scholze's notes, was proved.
					</li>
					<li class="fragment">
						Scholze said this was the part where he was worried about, and that he was now satisfied
						that
						the correctness of his result was checked.
					</li>
					<li class="fragment">
						About an year later, the origin goal, <strong>Theorem 9.1</strong> was proved in Lean.
					</li>
					<li class="fragment">Scholze said he understood aspects of his proof after the formalization which
						he hadn't earlier.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Sums of reciprocals; Ramsey theory</h4>
				<ul>

					<li class="fragment">
						In December 2021, Thomas Bloom proved that any set of natural numbers of positive upper
						density
						contains a finite subset whose reciprocals add up to $1$ (question of Erdős-Graham).
					</li>
					<li class="fragment">
						While his paper was still under review, Thomas Bloom
						learnt Lean, and,
						along with Bhavik Mehta, formalized this theorem in Lean.

					</li>
					<li class="fragment">
						Six months after a landmark paper (preprint) on Ramsey theory by Campos, Griffiths, Morris, and
						Sahasrabudhe
						appeared, Bhavik Mehta formalized their result in Lean.

					</li>


				</ul>
			</section>


			<section data-transition="convex" data-background-color="#002b36">
				<ul>
					<li>
						A proof in Lean is not only correct, but also:
						<ul>
							<li>Modular with precise abstraction boundaries that are fully respected.</li>
							<li>
								Each piece is correct exactly as stated.
							</li>
						</ul>
					</li>
					<li class="fragment">This facilitates focusing on different parts and different levels of details of
						the proof.</li>
					<li class="fragment">Collaboration is facilitated:
						<ul>
							<li class="fragment">Trust is not required to accept contributions.</li>
							<li class="fragment">Contributions are possible from those who only understand some
								ingredients.</li>
							<li class="fragment"> (Components of) Proofs suggested by Large Language Models such as
								GPT-4 can be used.</li>

						</ul>
					</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Equational Magmas: Terry Tao's experiment</h4>
				<ul>
					<li class="fragment">A <em>Magma</em> is a set $G$ with a single operation $\circ: G \times G \to
						G$.</li>
					<li class="fragment">We consider Magmas with a single equational axiom, such as $x \circ y = y
						\circ x$.</li>
					<li class="fragment">For all pairs of equational axioms using the Magma operation up to $4$ times,
						we ask whether the first axiom implies the second.</li>
					<li class="fragment">This was about 20 million questions.
					</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<ul>
					<li>These were studied in a collaboration using Lean's Zulip channel, and a Github
						workflow.
					</li>
					<li class="fragment">To prove an implication involves a deduction.
					</li>
					<li class="fragment">To disprove an implication we construct a <em>model</em> for the first axiom
						which does not satisfy the second.
					</li>
					<li class="fragment">Progress came from <em>deduction engines</em>, human ideas, and a remarkable
						interplay of these.
					</li>
					<li class="fragment">"The dog that didn't bark" &mdash; deep learning did not have a significant
						role.
					</li>
					<li class="fragment">Contrary to what many people excepted, <strong>every</strong> implication was
						either proved or disproved.
					</li>



				</ul>
			</section>




			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">
				<h2 style="color:#fffdd0">Automated Theorem Provers</h2>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h4>SAT (and SMT) Solvers</h4>
				<ul>
					<li class="fragment">
						The <strong>SAT</strong> (<em>boolean satisfiability</em>) problem asks whether we can solve
						finitely many logical equations involving boolean variables.
					<li class="fragment">By the Cook-Levine
						theorem, any finite problem in NP (easily verifiable) can be reduced to SAT.
					</li>
					<li class="fragment">SAT solvers, programs solving SAT, have been used to solve maths problems, for
						example:
					</li>
					<li class="fragment"><strong>Boolean Pythagorean triples problem:</strong> Can we colour
						each
						positive integers either red or blue, so that no Pythagorean triple of integers $a$, $b$, $c$,
						satisfying $a^{2}+b^{2}=c^{2}$ is monochromatic?</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h4>Resolution Theorem Provers</h4>
				<ul>
					<li class="fragment"><strong>Resolution Theorem Provers</strong> with <strong>Unification</strong>
						can solve problems in <em>first order logic</em>.
					</li>
					<li class="fragment">
						In principle, first-order logic includes all of mathematics; <span class="fragment">but, in
							practice,
							the proof must depend on a small number of axioms.</span>
					</li>
					<li class="fragment"> <strong>Robbins conjecture</strong> was a characterization of Boolean algebras
						in
						terms of
						associativity and commutativity of $\vee$ and the Robbins equation
						$\neg(\neg(a\vee b)\vee \neg(a \vee \neg b)) = a$.</li>
					<li class="fragment"> This was conjectured in the 1930s, and proved in 1996 by the
						automated theorem prover EQP.</li>
				</ul>
			</section>


			<section data-transition="zoom-in convex-out" data-background-image="sea.jpg">
				<h1>&nbsp;</h1>
				<h2 style="color:#fffdd0">Hybrid Systems:</h2>
				<h2 style="color:#fffdd0">Deep Learning</h2>
				<h4 style="color:#fffdd0">with</h4>
				<h2 style="color:#fffdd0">Automated/Interactive Provers</h2>

				<p>&nbsp;</p>
			</section>



			<section data-transition="convex" data-background="#b5533c">
				<h4>AlphaGeometry</h4>
				<ul>

					<li class="fragment">AlphaGeometry (from Google-Deepmind) is an AI system that can solve geometry
						problems.
					</li>
					<li class="fragment">More precisely, it can attempt to solve geometry problems which can be
						represented in a certain symbolic way,
						which includes most IMO geometry problems.
					</li>
					<li class="fragment">The score of AlphaGeometry on these problems was between the average score of
						the <strong>Silver medallists</strong> and the average score of the <strong>Gold
							medallists</strong> on those problems.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Components of AlphaGeometry</h4>
				<ul>

					<li class="fragment"><strong>Deductive Database + Algebraic Reasoning (DD+AR):</strong> a powerful
						and efficient deductive system for reasoning about geometry problems (extending earlier work).
					</li>
					<li class="fragment">A <strong>Language Model</strong> that suggests <em>auxiliary
							constructions.</em>
					</li>
					<li class="fragment">Generation of <strong>synthetic data</strong> to train the language model.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Deductive Database (+ Algebraic Reasoning)</h4>
				<ul>

					<li class="fragment">In a <strong>Deductive Database</strong>, we have (efficiently represented):
						<ul>
							<li>A finite collection of points.</li>
							<li>Relations between points, lines through them etc.; for example <code>perp A B C D</code>
							</li>
						</ul>
					</li>
					<li class="fragment">Rules of deduction let us deduce consequences.
					</li>
					<li class="fragment">Eg: $AB\perp BC$, $AB\perp BC$ gives $AB\parallel CD$.</li>
					<li class="fragment">Iterating deduction with all applicable rules till expansion halts
						gives all consequences that <strong>do not require</strong> adding a point.</li>
					<li class="fragment"><em>Algebraic Reasoning</em> added efficiency.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Synthetic data</h4>
				<ul>

					<li class="fragment">Millions of random configurations were generated, and all theorems were
						deduced.</li>
					</li>
					<li class="fragment">100 million theorems were found, for which minimal proofs were generated.</li>
					</li>
					<li class="fragment">Whenever the statement of a theorem did not involve a point but the proof did,
						the proof involved an <em>auxiliary construction</em> (10 million).</li>
					<li class="fragment">A language model ($\approx 150M$ params) was <em>pretrained</em> on all proofs
						and <em>fine-tuned</em> to predict auxiliary constructions.
					</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>AlphaGeometry: Solving Problems</h4>
				<ul>

					<li class="fragment">Given a geometry problem represented symbolically, all deductions are made
						using DD+AR.</li>
					<li class="fragment">If the deductions do not include the final conclusion, then the language model
						is used to generate candidate auxiliary constructions.</li>
					</li>
					<li class="fragment">For (some of) the auxiliary constructions, all deductions are made after the
						construction.</li>
					<li class="fragment">This process is repeated in a <strong>beam search</strong>.</li>
				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background-image="AlphaGeomPfs.jpg"
				data-background-size="contain">
			</section>


			<section data-transition="convex" data-background="#b5533c">
				<h4>AlphaProof</h4>
				<ul>

					<li class="fragment">In <strong>AlphaProof</strong> (from Google-Deepmind) the special deductive
						system of AlphaGeometry is replaced by the Lean theorem prover.
					</li>
					<li class="fragment">This means AlphaProof can in principle attempt problems in all of mathematics.
					</li>
					<li class="fragment">The training data was obtained by <em>Autoformalization</em>, with failed
						translations rejected and consistent but wrong translations still valid for training.
					</li>
					<li class="fragment">When run for two days, AlphaProof obtained an IMO score
						slightly below a <strong>Gold
							medal</strong>.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>PatternBoost</h4>
				<ul>

					<li class="fragment">This work searches for <em>patterns</em> in a general sense (such as graphs
						with some special properties), finding new counterexamples or extrema.
					</li>
					<li class="fragment">This combines two steps: a <em>local search</em> specific to the problem and a
						custom transformer model.
					</li>
					<li class="fragment">The local search is used to generate initial data of promising examples.
					</li>
					<li class="fragment">Using these, a <em>more-like-this</em> neural network is trained to generate
						qualitatively similar examples.</li>
					<li class="fragment">We locally optimize these to get new candidates, and train with the globally
						best examples.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Querying Foundation models</h4>
				<ul>
					<li class="fragment">GPT-4 could solve:</strong> Find all $2 \times 2$ matrices with rational
						entries such that $A^5 = I$.</li>
					<li class="fragment">However, it persistently made mistakes with some much easier problem.</li>
					<li class="fragment">Robert Ghrist asked <strong>o1</strong> to generalise the bottleneck version of
						the max-flow min-cut theorem, to networks whose edges have values in some ordered or
						partially-ordered algebraic structure more general than real numbers. It was successful.</li>
					<li class="fragment"> Some examples with <a href="https://gemini.google.com/app/2f44749b24289e5c"
							target="_blank">Gemini 2.0 Flash</a>.
					</li>

				</ul>
			</section>


			<section data-transition="zoom" data-background="hills.png">
				<h2 style="color:#fffdd0">Automated Theorem Proving</h2>
				<h4 style="color:#fffdd0">and</h4>
				<h2 style="color:#fffdd0">Mathematics</h2>

			</section>



			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>Large Language Models have remarkable abilities, roughly corresponding to our
						<em>intuition</em>.
						<ul>
							<li class="fragment">They can reason by analogy.</li>
							<li class="fragment">This includes mixing and matching concepts from different domains.
							</li>
							<li class="fragment">They are excellent at Natural Language.</li>
							<li class="fragment">They are familiar with a huge amount of stuff.</li>
							<li class="fragment">They can combine their broad knowledge with specific knowledge
								from their input.</li>
						</ul>
					</li>
					<li class="fragment">Language models can effectively use <em>agents</em> such as web searches and
						Python interpreters.</li>
				</ul>
			</section>

			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>LLMs can generate <em>candidates</em> for:
						<ul>
							<li class="fragment">Full proofs if reasonably small.</li>
							<li class="fragment">Problems given a theme.</li>
							<li class="fragment">Proof strategies.</li>
						</ul>
					</li>
					<li class="fragment">In each case we can further use an LLM to translate to a formal proof and/or
						write code for experiments.</li>
					<li class="fragment">This allows autonomous development and improvement by generating synthetic data
						or accumulating knowledge.</li>
					<li class="fragment">Currently, LLMs have weaknesses such as needing more in-context data than
						humans.</li>
				</ul>
			</section>

			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>Consider a scenario in the future where a fully autonomous AI system can prove theorems,
						formalize them to ensure correctness, and write a paper with these theorems:
						<ul>
							<li class="fragment">At a level at least as good as 90% of the work of 90% of
								mathematicians.</li>
							<li class="fragment">With a paper written every few days, or hours, or minutes.
							</li>
						</ul>
					</li>
					<li class="fragment">We can try to react defensively, trying to detect and ban such stuff or focus
						mathematics on prestige (a Veblen good) rather than as knowledge (a public good).</li>
				</ul>
			</section>

			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>There are strong motives for AI companies and researchers to work on the (demonstrable)
						mathematical capabilities of AI.
					</li>
					<li class="fragment">However, they have little motivation to work on tools and workflows that make
						these useful for mathematicians.</li>
					<li class="fragment">Science has benefitted enormously from AI systems: AlphaFold 2 (and 3),
						controlling nuclear fusion, weather prediction.</li>
					<li class="fragment">Mathematics has the opportunity to grow in collaboration with AI, and the
						danger of shrinking in relevance if overtaken by autonomous AI.</li>

				</ul>
			</section>


	</div>
	</div>

	<script src="js/reveal.js"></script>


	<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/concave/concave/zoom


			math: {
				mathjax: '../MathJax/MathJax.js',
				//				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
				config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
			},

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				// { src: '../highlight/highlight.pack.js', async: true, condition: function () { return !!document.querySelector('pre code'); }, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});

		//			Reveal.addEventListener( 'slidechanged', function( event ) {
		//				MathJax.Hub.Rerender();
		//			} );

	</script>

</body>

</html>