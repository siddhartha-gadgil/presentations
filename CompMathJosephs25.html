<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Computers for Mathematics</title>

	<meta name="description" content="ICTS 2024 summer school lecture">
	<meta name="author" content="Siddhartha Gadgil">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport"
		content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">
	<link rel="stylesheet" href="kwk.css">

	<link rel="icon" href="IIScLogo.jpg">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside this container is displayed as a slide -->
		<section class="slides">
			<section data-transition="slide">
				<h2>Computers for Mathematics:</h2>
				<h2>Past, Present and Future </h2>
				<h4>Siddhartha Gadgil</h4>
				<p>Department of Mathematics</p>
				<p>Indian Institute of Science, Bangalore</p>


			</section>

			<section>
				<h4>Eras for Computing in mathematics</h4>
				<ul>
					<li> Looking back from early April 2025: </li>
					<li class="fragment"> <strong>Computers</strong> (about 70 years): Computation, Symbolic
						Computation, Symbolic AI.</li>
					<li class="fragment"> <strong>Deep Learning:</strong> 10-15 years.</li>
					<li class="fragment"> <strong>ChatGPT (large-scale pretraining)</strong>: about 2 and a half years.
					</li>
					<li class="fragment"> Many improvements: better models, <em>agents</em>. </li>
					<li class="fragment"> <strong>"Thinking" models</strong>: 2-6 months.</li>
					<li class="fragment"> <strong>"Thinking"+ large-scale pretraining:</strong> about 2-3 weeks.</li>
				</ul>
			</section>

			<section data-transition="zoom-in slide-out" data-background-color="teal">
				<h1>(Mathematical) Reasoning</h1>
			</section>

			<section data-background-color="teal">
				<h4> A proof: Sum of Angles in a Triangle</h4>
				<ul>
					<li class="fragment"> Consider a triangle $ABC$.</li>
					<img class="fragment" src="angle_sum.png" height="250" />
					<li class="fragment"> Draw a line through $A$ parallel to $BC$.</li>
					<li class="fragment"> Use equality of interior opposite angles.</li>
					<li class="fragment"> Use angles on a line add up to 180 degrees.</li>

				</ul>
			</section>
			<section data-background-color="teal">
				<h4>Another proof: infinitely many primes</h4>
				<ul>
					<li class="fragment"> For any $n$, we show that there exists a prime $p$ such that $p > n$.</li>
					<li class="fragment"> Consider the number $n!+1$, where $n! = 1 \times 2 \times \dots \times n$.
					</li>
					<li class="fragment"> Let $p$ be the smallest prime factor of $n!+1$.
					</li>
					<li class="fragment"> If $p \leq n$ then $p | n!$ so $p \nmid n!+1$.</li>
					<li class="fragment"> Hence $p$ is a prime greater than $n$.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">What went into the proofs</h4>
				<ul>
					<li class="fragment">
						<p>Auxiliary Constructions:</p>
						<ul>
							<li class="fragment">the parallel line</li>
							<li class="fragment">the term $n! + 1$.</li>
						</ul>
					</li>
					<li class="fragment">
						<p>Theorems/Definitions to use.</p>
					</li>
					<li class="fragment">
						<p>Deductions and Computations.</p>
					</li>
					<li class="fragment">
						<p>The theorems and definitions needed both knowledge of the <strong>precise statement</strong>
							and the ability to <strong>recognize relevance</strong>.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal">
				<h4 id="two-systems-for-thinking">Two systems for Thinking</h4>
				<p class="fragment">Our thinking is based on two systems:</p>
				<ul>
					<li class="fragment">System 1: Intuitive, Associative, Automatic.
						<ul>
							<li class="fragment">handles language, vision, sensorimotor tasks.</li>
							<li class="fragment">fast, but error prone.</li>
							<li class="fragment">In mathematics: auxiliary constructions, lemmas, relevant results.</li>
						</ul>
					</li>
					<li class="fragment">System 2: Analytical, Logical, Controlled.
						<ul>
							<li class="fragment">handles reasoning, planning.</li>
							<li class="fragment">slow, but accurate.</li>
							<li class="fragment">In mathematics: computations, deductions.</li>
						</ul>
					</li>
				</ul>

			</section>
			<section data-background-color="#002b36">
				<h4 id="moravec-paradox">Moravec Paradox</h4>
				<ul>
					<li class="fragment">System 1 (including <em>sensorimotor system</em>), needs far more raw
						computational power than System 2.</li>
					<li class="fragment">Our brain (mostly System 1) performs about 100 trillion operations per second
						(about 100
						Teraflops) according
						to Moravec, far above the computation available in the 1990s.</li>
					<li class="fragment">Evolution has optimized our brain for many system-1 tasks.</li>
					<li class="fragment">We greatly underestimate/overestimate the difficulty of specific tasks for
						computers: based on our effort and the gap with experts.</li>
				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background="kepler.png" data-state="dimbg">
				<h1>(Classical) Computer Proofs</h1>
				<h2>in</h2>
				<h1>Mathematics</h1>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h3>Some computer-assisted proofs</h3>
				<ul>
					<li class="fragment"><strong>Four-colour problem:</strong> Any map can be coloured with at most $4$
						colours.</li>
					<li class="fragment"><strong>Kepler Conjecture:</strong> The most efficient way to pack spheres is
						the hexagonal close packing.</li>
					<li class="fragment"><strong>Boolean Pythagorean triples problem:</strong> Is it possible to colour
						each of the
						positive integers either red or blue, so that no Pythagorean triple of integers $a$, $b$, $c$,
						satisfying $a^{2}+b^{2}=c^{2}$ are all the same color?</li>
					<li class="fragment">All these proofs are long (perhaps unavoidable).</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h3> Robbins conjecture </h3>
				<ul>
					<li class="fragment"> Robbins conjecture was a conjectural characterization of Boolean algebras in
						terms of
						associativity and commutativity of $\vee$ and the Robbins equation
						$\neg(\neg(a\vee b)\vee \neg(a \vee \neg b)) = a$.</li>
					<li class="fragment"> This was conjectured in the 1930s, and finally proved in 1996 using the
						automated theorem prover <strong>EQP</strong>.</li>
					<li class="fragment"> So far, this seems to be the only major success of deductive theorem provers.
					</li>
			</section>

			<section data-transition="zoom-in concave-out" data-background="alpha-zero.jpg">
				<h1>&nbsp;</h1>
				<h2 style="color:#fffdd0">Stages of</h2>
				<h1 style="color:#fffdd0">Artificial Intelligence</h1>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="common-sense">Symbolic AI</h4>
				<ul>
					<li class="fragment">This has been developed since the 1950s.</li>
					<li class="fragment">These are purely <strong>rule-based</strong> systems.</li>
					<li class="fragment">At the core of such <strong>Expert Systems</strong> are good algorithms and ways to organize data to search very fast.</li>
					<li class="fragment">Besides knowledge and fast algorithms, for success we also need enough
						<strong>raw power</strong>.
					</li>
					<li class="fragment">Deep Blue, which defeated Gary Kasparov, was an <em>Expert system</em>, based on elaborate rules for judging
						strengths of positions.
					</li>
			</section>


			<section data-background-color="teal" data-transition="concave">
				<h4 id="tacit-knowledge">Deep Learning: tacit knowledge and intuition</h4>
				<ul>
					<li class="fragment"><strong>Tacit knowledge</strong> is the kind of knowledge that is difficult to
						transfer to another person by
						means of writing it down or verbalizing it.</li>
					<li class="fragment">Examples: riding a bicycle, speaking a language; evaluating positions in chess.
					</li>
					<li class="fragment">Tacit knowledge is learnt from examples, as are other aspects of intuition.</li>
					<li class="fragment"><strong>Deep Learning</strong> allowed efficient learning from data, once the raw power was enough (which happened aroung 2012).</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<ul>
					<li><strong>Reinforcement Learning:</strong> Try different things (e.g. moves in a game) and learn from what works: no need of data. </li>
					<li class="fragment"><strong>Representation Learning:</strong> Learn a good <em>semantic</em> representation of the data, and a good output based on the representation. Can use synthetic tasks for the first step.</li>
					<li class="fragment"><strong>Transfer Learning:</strong> Use the same represention for many tasks. Training on one helps with the others.</li>
					<li class="fragment"><strong>Pretraining:</strong> Learn a representation on a massive scale for a single task: predict the next word. Was possible because of the <em>transformer</em> architecture.</li>
				</ul>
			</section>

			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">
				<h1 style="color:#fffdd0">The ChatGPT Era</h1>
			</section>


			<section data-transition="convex" data-background-color="#002b36">
				<ul>
					<li>
						The world was introduced to Large Language Models through ChatGPT, powered by GPT-3.5.
						<ul>
							<li class="fragment">This could prove that there are infinitely many primes.</li>
							<li class="fragment">
								But tried to use the same proof for <em>infinitely many primes ending in $7$</em>,
								giving rubbish.
							</li>
						</ul>
					</li>
					<li class="fragment">Models got better: GPT-4 turbo correctly proves that there are infinitely many
						primes ending in $7$.</li>
					<li class="fragment">Further, models were aided by agents: web search, Python interpreters,
						<em>function calling</em>.
					</li>
					<li class="fragment">The next level are thinking models: o1, o3, DeepSeek-R1,
						Gemini-2-flash-thinking etc.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Linear Algebra by GPT-4</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^2$
						is invertible then $A$ is invertible.</li>
					<li class="fragment">3 solutions: all correct and different.</li>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^2$
						is diagonalisable then $A$ is diagonalisable.</li>
					<li class="fragment">Pointed out that this was <em>false</em> and gave a counterexample.</li>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^3=I$
						then $A$ is diagonalisable.</li>
					<li class="fragment">Answered correctly.</li>
			</section>

			<section data-background-color="#002b36">
				<h4>A harder problem</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Find all $2 \times 2$ matrices with rational entries
						such that
						$A^5 = I$.</li>
						<li class="fragment">
							Answered correctly: $A = I$.
						</li>
					<li class="fragment">Also answered the follow-up algebra/number theory question.</li>
					
			</section>


			<section data-background-color="#002b36">
				<h4>An Analysis question</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth
						function such that $f''(x) > 0$ for all $x \in \mathbb{R}$. Prove that $f$ is unbounded.</li>
					<li class="fragment">Persistently wrong answers, even with guidance like split into cases.</li>
					<li class="fragment">Proofs either used <em>every increasing sequence is unbounded</em> or assumed
						that the maximum is attained for the functions.</li>
			</section>

			<section data-background-color="#002b36">
				<h4>DeepSeek-R1 thinking: A harder problem</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Suppose $X$, $Y$ and $Z$ are random variables such
						that $X\sim N(1, 1)$, $Y\sim Beta(2, 2)$ and $Z\sim Uniform[0, 3]$. Find $P(X + Y > Z)$.</li>
					<li class="fragment">Answered correctly: $P(X + Y > Z) = \frac{1}{2}$.</li>
					<li class="fragment">The model took 808 seconds, first trying many kinds of tricks to break up and integrate. 
					<li class="fragment">A little after 10 minutes it spotted the key idea: use <em>symmetry</em>.
						</li>
						<li class="fragment">It then spent some time trying to reconcile the answer half with its previous answer 0.58.</em>
						</li>
					</span>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>The model then decided "The mistake in the previous calculation was a result of incorrect
						integration steps, likely due to <em>human error</em> during the complex integration by parts
						and substitutions."</li>
					<li class="fragment">However, DeepSeek-R1 struggled with the Analysis problem mentioned earlier.
					</li>
					<li class="fragment">The proof it gave split into cases and analysed these, but missed or messed up
						cases.</li>
					<li class="fragment">After 3-4 rounds of pointing out errors, it got to the proof.</li>
				</ul>
			</section>



			<section data-background-color="#002b36">
				<h4>Research Help: Gemini-2-flash-thinking</h4>
				<ul>
					<li class="fragment"><strong>Query</strong> (approx.) A research student is working on Poisson
						boundaries of groups with negative curvature. Suggest background material, useful results, and
						specific problems to work on.</li>
					<li class="fragment">Gave very useful answers, including constructions and theorems I did not know
						about.
					</li>
					<li class="fragment">Could further explain when asked about these.</li>
					<li class="fragment">Also helpful with more specific questions, other areas etc and got helpful (but
						not always accurate) answers.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Gemini-2.5-pro</h4>

				<ul>
					<li class="fragment">The analysis problem was answered cleanly by Gemini-2.5-pro.</li>
					<li class="fragment">It also solved the other questions I asked it, including the probability problem in 40 seconds.</li>
					<li class="fragment">With some back-and-forth, it helped me in research, giving a counterexample to
						a conjecture I made.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Complementing AI?</h4>
				<ul>
					<li class="fragment">We would like to <em>automatically</em> check the output of the language models
						and provide feedback.</li>
					<li class="fragment">We would also want an automatic workflow, involving decomposing problems,
						trying different things, accumulating knowledge etc.
					</li>
					<li class="fragment">Such systems should be integrated into human workflows.</li>
					<li class="fragment">So called <em>Formalization</em> and <em>Autoformalization</em> can facilitate
						this.</li>
						<li class="fragment">Seize the day!</li>
			</section>



			<section data-transition="zoom" data-background="hills.png">
				<h2 style="color:#fffdd0">Artificial Intelligence</h2>
				<h4 style="color:#fffdd0">and</h4>
				<h2 style="color:#fffdd0">Mathematics</h2>

			</section>


			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>Consider a scenario in the future where a fully autonomous AI system can prove theorems,
						formalize them to ensure correctness, and write a paper with these theorems:
						<ul>
							<li class="fragment">At a level at least as good as 90% of the work of 90% of
								mathematicians.</li>
							<li class="fragment">On an industrial scale.
							</li>
						</ul>
					</li>
					<li class="fragment">Or tools are available to let people with limited mathematical skills
						produce
						such mathematics.</li>
					<li class="fragment">We can try to react defensively: try to detect and ban AI or focus
						mathematics on prestige (a Veblen good) rather than as knowledge (a public good).</li>
				</ul>
			</section>

			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>There are strong motives for the AI community to work on the
						mathematical capabilities of AI: as <strong>measurable goals/tests for reasoning</strong>.
					</li>
					<li class="fragment">However, they have little motivation to work on <strong>tools</strong> that
						make
						these useful for <strong>mathematicians</strong>.</li>
					<li class="fragment">Science has benefitted enormously from AI systems: AlphaFold 2 (and 3),
						controlling nuclear fusion, weather prediction.</li>
					<li class="fragment">Mathematics has the opportunity to grow in collaboration with AI, and the
						danger of shrinking in relevance if overtaken by autonomous AI.</li>

				</ul>
			</section>
		</section>
		<section data-background-image="LFCM2025.png" data-background-size="45%"></section>

	</div>
	</div>

	<script src="js/reveal.js"></script>


	<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/concave/concave/zoom


			math: {
				mathjax: '../MathJax/MathJax.js',
				//				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
				config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
			},

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				// { src: '../highlight/highlight.pack.js', async: true, condition: function () { return !!document.querySelector('pre code'); }, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});

		//			Reveal.addEventListener( 'slidechanged', function( event ) {
		//				MathJax.Hub.Rerender();
		//			} );

	</script>

</body>

</html>