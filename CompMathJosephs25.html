<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Computers doing Mathematics</title>

	<meta name="description" content="ICTS 2024 summer school lecture">
	<meta name="author" content="Siddhartha Gadgil">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport"
		content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">
	<link rel="stylesheet" href="kwk.css">

	<link rel="icon" href="IIScLogo.jpg">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside this container is displayed as a slide -->
		<section class="slides">
			<section data-transition="slide">
				<h2>Computers doing Mathematics </h2>
				<h4>Siddhartha Gadgil</h4>
				<p>Department of Mathematics</p>
				<p>Indian Institute of Science, Bangalore</p>


			</section>

			<section>
				<h4>Eras for Computing in mathematics</h4>
				<ul>
					<li> Looking back from early April 2025: </li>
					<li class="fragment"> <strong>Computers</strong> (about 70 years): Computation, Symbolic
						Computation, Symbolic AI .</li>
					<li class="fragment"> <strong>Deep Learning:</strong> 10-15 years.</li>
					<li class="fragment"> <strong>ChatGPT (large-scale pretraining)</strong>: about 2 and a half years.
					</li>
					<li class="fragment"> Many improvements: better models, <em>agents</em>. </li>
					<li class="fragment"> <strong>"Thinking" models</strong>: 2-6 months.</li>
					<li class="fragment"> <strong>"Thinking"+ Large-scale pretraining:</strong> about 2-3 weeks.</li>
				</ul>
			</section>

			<section data-transition="zoom-in slide-out" data-background-color="teal">
				<h1>(Mathematical) Reasoning</h1>
			</section>

			<section data-background-color="teal">
				<h4> A proof: Sum of Angles in a Triangle</h4>
				<ul>
					<li class="fragment"> Consider a triangle $ABC$.</li>
					<img class="fragment" src="angle_sum.png" height="250" />
					<li class="fragment"> Draw a line through $A$ parallel to $BC$.</li>
					<li class="fragment"> Use equality of interior opposite angles.</li>
					<li class="fragment"> Use angles on a line add up to 180 degrees.</li>

				</ul>
			</section>
			<section data-background-color="teal">
				<h4>Another proof: infinitely many primes</h4>
				<ul>
					<li class="fragment"> For any $n$, we show that there exists a prime $p$ such that $p > n$.</li>
					<li class="fragment"> Consider the number $n!+1$, where $n! = 1 \times 2 \times \dots \times n$.
					</li>
					<li class="fragment"> Let $p$ be the smallest prime factor of $n!+1$.
					</li>
					<li class="fragment"> If $p \leq n$ then $p | n!$ so $p \nmid n!+1$.</li>
					<li class="fragment"> Hence $p$ is a prime greater than $n$.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">What went into the proofs</h4>
				<ul>
					<li class="fragment">
						<p>Auxiliary Constructions:</p>
						<ul>
							<li class="fragment">the parallel line</li>
							<li class="fragment">the term $n! + 1$.</li>
						</ul>
					</li>
					<li class="fragment">
						<p>Theorems/Definitions to use.</p>
					</li>
					<li class="fragment">
						<p>Deductions and Computations.</p>
					</li>
					<li class="fragment">
						<p>The theorems and definitions needed both knowledge of the <strong>precise statement</strong>
							and the ability to <strong>recognize relevance</strong>.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal">
				<h4 id="two-systems-for-thinking">Two systems for Thinking</h4>
				<p class="fragment">Our thinking is based on two systems:</p>
				<ul>
					<li class="fragment">System 1: Intuitive, Associative, Automatic.
						<ul>
							<li class="fragment">handles language, vision, other sensorimotor tasks.</li>
							<li class="fragment">fast, but error prone.</li>
						</ul>
					</li>
					<li class="fragment">System 2: Analytical, Logical, Controlled.
						<ul>
							<li class="fragment">handles reasoning, planning, problem solving.</li>
							<li class="fragment">slow, but accurate.</li>
						</ul>
					</li>
				</ul>

			</section>
			<section data-background-color="#002b36">
				<h4 id="moravec-paradox">Moravec Paradox</h4>
				<ul>
					<li class="fragment">System 1 (including <em>sensorimotor system</em>), needs far more raw
						computational power than System 2.</li>
					<li class="fragment">Our brain (mostly System 1) performs about 100 trillion operations per second
						(about 100
						Teraflops) according
						to Moravec, far above the computation available in the 1990s.</li>
					<li class="fragment">Evolution has optimized our brain for many system-1 tasks.</li>
					<li class="fragment">We greatly underestimate/overestimate the difficulty of specific tasks for
						computers: based on our effort and the gap with experts.</li>
				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background="kepler.png" data-state="dimbg">
				<h1>(Classical) Computer Proofs</h1>
				<h2>in</h2>
				<h1>Mathematics</h1>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h3>Some computer-assisted proofs</h3>
				<ul>
					<li class="frag-ment"><strong>Four-colour problem:</strong> Any map can be coloured with at most $4$
						colours.</li>
					<li class="frag-ment"><strong>Kepler Conjecture:</strong> The most efficient way to pack spheres is
						the hexagonal close packing.</li>
					<li class="frag-ment"><strong>Boolean Pythagorean triples problem:</strong> Is it possible to colour
						each of the
						positive integers either red or blue, so that no Pythagorean triple of integers $a$, $b$, $c$,
						satisfying $a^{2}+b^{2}=c^{2}$ are all the same color?</li>
					<li class="frag-ment">All these proofs are long (perhaps unavoidable).</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h3> Robbins conjecture </h3>
				<ul>
					<li class="frag-ment"> Robbins conjecture was a conjectural characterization of Boolean algebras in
						terms of
						associativity and commutativity of $\vee$ and the Robbins equation
						$\neg(\neg(a\vee b)\vee \neg(a \vee \neg b)) = a$.</li>
					<li class="frag-ment"> This was conjectured in the 1930s, and finally proved in 1996 using the
						automated theorem prover <strong>EQP</strong>.</li>
					<li class="frag-ment"> So far, this seems to be the only major success of deductive theorem provers.
					</li>
			</section>

			<section data-transition="zoom-in concave-out" data-background="alpha-zero.jpg">
				<h1>&nbsp;</h1>
				<h2 style="color:#fffdd0">Stages of</h2>
				<h1 style="color:#fffdd0">Artificial Intelligence</h1>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="common-sense">The common sense barrier</h4>
				<ul>
					<li class="fragment">We know a lot of things, on which we base <em>common sense</em> (according to
						Marvin Minsky).</li>
					<li class="fragment">Further, these pieces of knowledge are organized in a way that they can be
						combined.</li>
					<li class="fragment"><strong>Expert Systems</strong> have a lot of knowledge encoded as structured
						data, functions,
						algorithms.</li>
					<li class="fragment">These are purely <strong>rule-based</strong> systems.</li>
					<li class="fragment">Besides knowledge and fast algorithms, for success we also need enough
						<strong>raw power</strong>.
					</li>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Kasparov vs Deep Blue</h4>
				<ul>
					<li class="fragment">In 1997, a computer <strong>Deep Blue</strong> defeated the Chess world
						champion Gary Kasparov.
					</li>
					<li class="fragment"> Deep Blue was an <em>Expert system</em>, based on elaborate rules for judging
						strengths of positions.
					</li>
					<li class="fragment">However, Deep Blue was very limited in certain
						<strong>capabilities</strong>.
					</li>
					<li class="fragment"> Kasparov was far better at evaluating strengths of positions than Deep Blue,
						and in knowing what moves are worth considering (intuition). </li>
					<li class="fragment"> Deep Blue compensated for this by being able to consider far more move
						sequences.</li>
				</ul>

			</section>


			<section data-background-color="teal" data-transition="concave">
				<h4 id="tacit-knowledge">The tacit knowledge barrier</h4>
				<ul>
					<li class="fragment"><strong>Tacit knowledge</strong> is the kind of knowledge that is difficult to
						transfer to another person by
						means of writing it down or verbalizing it.</li>
					<li class="fragment">Examples: riding a bicycle, speaking a language; evaluating positions in chess.
					</li>
					<li class="fragment">Tacit knowledge is much more important in the chinese game Go compared to
						Chess.</li>
					<li class="fragment">Further, in Go the number of legal moves is much larger so brute force is less
						effective.</li>
					<li class="fragment">As a result, DeepBlue style expert systems are hopeless at Go.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Deep learning</h4>
				<ul>
					<li class="fragment">Computers can acquire knowledge by
						<em>learning</em>.
					</li>
					<li class="fragment">The idea of computers learning goes back at least to Turing, or even regression
						in statistics.
					</li>
					<li class="fragment">
						What was needed was hardware and a system that could learn efficiently: <em>Deep neural
							networks.</em>
					</li>
					<li class="fragment">In a deep neural network the input is transformed into the output through a
						series of
						layers.</li>
					<li class="fragment">Each layer is typically a linear transformation, depending on weights, followed
						by a simple
						non-linear function.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<img height="250" src="DeepNeuralNet.png" />
				<ul>
					<li class="fragment">Starting about 2012, deep neural networks performed very well in image
						processing tasks, far
						above rule-based systems.</li>
					<li class="fragment">

						It turns out that as the image is processed by successive layers, the raw data gets modified
						to capture abstraction/meaning.
					</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="limitations-as-of-about-ten-years-ago">Limitations (≈ 10-15 years ago)</h4>
				<ul>
					<li class="fragment">Needed labelled data for training.</li>
					<li class="fragment">Narrow: training specific to the task.</li>
					<li class="fragment">Had difficulty with long range dependencies, in particular language.</li>
					<li class="fragment">Hard to scale up, parallelize.</li>
					<li class="fragment">Could not go beyond training data.</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="representation-learning">Representation Learning</h4>
				<ul>
					<li class="fragment">
						<p><strong>Word2Vec</strong> considered the problem of predicting a word given its neighbours.
						</p>
					</li>
					<li class="fragment">
						<p>This was trained on solutions of this problem that involve mapping words into space, and
							predicting
							from neighbours using the points.</p>
					</li>
					<li class="fragment">
						<p>Only the mapping of words into space was retained, so <em>geometry</em> captures
							<em>associations</em>.
						</p>
					</li>
					<li class="fragment">
						<p>Another lesson is that it is useful to solve synthetic problems: play, solving exercises.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="transfer-learning">Transfer learning</h4>
				<ul>
					<li class="fragment">A set of common layers was used for many tasks.</li>
					<li class="fragment">Some of the tasks could be synthetic (exercises).</li>
					<li class="fragment">This resulted in networks trained for one task needing much less training for a
						related task.
					</li>
				</ul>

			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4>AlphaGo, AlphaZero</h4>
				<ul>
					<li class="fragment"> In March 2016, a Go playing system AlphaGo defeated 18-time world
						champion Lee Sedol;
						in January 2017 defeated the world #1 Ke Jie.
					</li>
					<li class="fragment"> The strategy of AlphaGo was based on
						<ul>
							<li> A <em>policy</em> function - what moves to consider. </li>
							<li> A <em>value</em> function - how good a position is. </li>
						</ul>
					</li>
					<li class="fragment"> Both these were learned by deep neural networks.</li>
					<li class="fragment"> The policy network was initially trained by learning to predict the next
						move from games of expert players (<em>behaviour cloning</em>).</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<ul>
					<li> The value network was trained by AlphaGo playing against
						itself: <strong>Reinforcement Learning</strong>.
					</li>
					<li class="fragment"> Successors AlphaGoZero and AlphaZero learnt purely by self-play, and the
						latter played Chess.</li>
					<li class="fragment">AlphaZero took just 4 hours to become the strongest chess player on the
						planet
						(beating a traditional chess program, Stockfish).</li>
					<li class="fragment">AlphaZero &ldquo;had a dynamic, open style&rdquo;, and &ldquo;prioritizes
						piece
						activity over material, preferring positions that looked risky and aggressive.&rdquo;</li>
				</ul>
			</section>


			<section data-background-color="teal" data-transition="concave">
				<h4 id="attention-is-all-you-need">“Attention is all you need”</h4>
				<ul>
					<li class="fragment">The meaning of a word depends on the context, i.e., other words surrounding it.
					</li>
					<li class="fragment">In the <strong>transformer architecture</strong>, this is captured by learning
						to which other
						words to pay attention.</li>
					<li class="fragment">This allowed parallelization and captured long-range dependencies.</li>
					<li class="fragment">Transformers are used in <strong>AlphaFold 2</strong> which predicts protein
						structures on par with experiments.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Foundation Models</h4>
				<ul>
					<li class="fragment"> With the transformer architecture training could be scaled up enormously.</li>
					<li class="fragment"> It was found that models kept improving as the number of parameters grew (when
						trained on huge datasets).</li>
					<li class="fragment"> Huge models such as GPT-4, Claude Sonnet, Gemini, Grok, Mistral, DeepSeek etc
						have been trained on essentially all of the
						internet by trying to predict the next word.
					</li>

				</ul>
			</section>


			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">
				<h1 style="color:#fffdd0">The ChatGPT Era</h1>
			</section>


			<section data-transition="convex" data-background-color="#002b36">
				<ul>
					<li>
						The world was introduced to Large Language Models through ChatGPT, powered by GPT-3.5.
						<ul>
							<li class="fragment">This could prove that there are infinitely many primes.</li>
							<li class="fragment">
								But tried to use the same proof for <em>infinitely many primes ending in $7$</em>,
								giving rubbish.
							</li>
						</ul>
					</li>
					<li class="fragment">Models got better: GPT-4 turbo correctly proves that there are infinitely many
						primes ending in $7$.</li>
					<li class="fragment">Further, models were aided by agents: web search, Python interpreters,
						<em>function calling</em>.
					</li>
					<li class="fragment">The next level are thinking models: o1, o3, DeepSeek-R1,
						Gemini-2-flash-thinking etc.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Linear Algebra by GPT-4</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^2$
						is invertible then $A$ is invertible.</li>
					<li class="fragment">3 solutions: all correct and different.</li>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^2$
						is diagonalisable then $A$ is diagonalisable.</li>
					<li class="fragment">Pointed out that this was <em>false</em> and gave a counterexample.</li>
					<li class="fragment"><strong>Problem:</strong> Let $A$ be a square matrix. Prove that if $A^3=I$
						then $A$ is diagonalisable.</li>
					<li class="fragment">Answered correctly.</li>
			</section>

			<section data-background-color="#002b36">
				<h4>A harder problem</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Find all $2 \times 2$ matrices with rational entries
						such that
						$A^5 = I$.</li>
					<span class="fragment">
						<li><strong>Proof:</strong>
							A $2 \times 2$ matrix $A$ over the rationals is of the form

							\[
							A = \begin{pmatrix}
							a & b \\
							c & d
							\end{pmatrix},
							\]

							where $a, b, c, d \in \mathbb{Q}$. </li>
						<li>
							We want to find all such $A$ such that $A^5 = I$, where $I$ is the $2 \times 2$ identity
							matrix.
						</li>
					</span>
			</section>

			<section data-background="#002b36">
				<ul>
					<li>We can start by noting that the minimal polynomial of such an $A$ must divide $x^5 - 1 = 0$.
						Over the rationals, this polynomial factors into irreducible factors as

						\[
						x^5 - 1 = (x - 1)(x^4 + x^3 + x^2 + x + 1).
						\]
					</li>
					<li>
						So the minimal polynomial of $A$ must either be $x - 1$ or $x^4 + x^3 + x^2 + x + 1 = 0$.
					</li>
					<li>
						However, the second polynomial has degree $4$ and the minimal polynomial of a $2 \times 2$
						matrix can have degree at most $2$, so $A$ must satisfy $x - 1 = 0$. That is, $A = I$.
					</li>
					<li>
						To verify our claim, we note that if $A = I$, then indeed $A^5 = I^5 = I$. So the only $2 \times
						2$ matrix over the rationals such that $A^5 = I$ is $A = I$.
					</li>
				</ul>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>
						To verify our claim, we note that if $A = I$, then indeed $A^5 = I^5 = I$. So the only $2 \times
						2$ matrix over the rationals such that $A^5 = I$ is $A = I$.
					</li>
					<li>
						<strong>Problem:</strong> Prove that the polynomial $x^4 + x^3 + x^2 + x + 1$ is irreducible
						over the rationals.
					</li>
					<li>
						To prove that the polynomial $x^4 + x^3 + x^2 + x + 1$ is irreducible over the rationals, one
						approach is by using a substitution and then applying Eisenstein's Criterion, a well-known
						theorem in number theory that provides a sufficient condition for a polynomial ...
					</li>

				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Analysis questions</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth
						function such that $f''(x) > 0$ for all $x \in \mathbb{R}$. Prove that $f$ is unbounded.</li>
					<li class="fragment"><strong>Prove or disprove:</strong> Suppose $f:\mathbb{R} \to \mathbb{R}$ is a
						smooth function such that $f'(x) > 0$ for all $x \in \mathbb{R}$. Then $f$ is unbounded.
					</li>
					<li class="fragment">Persistently wrong answers, even with guidance like split into cases.</li>
					<li class="fragment">Proofs either used <em>every increasing sequence is unbounded</em> or assumed
						that the maximum is attained for the functions.</li>
			</section>

			<section data-background-color="#002b36">
				<h4>DeepSeek-R1 thinking: A harder problem</h4>
				<ul>
					<li class="fragment"><strong>Problem:</strong> Suppose $X$, $Y$ and $Z$ are random variables such
						that $X\sim N(1, 1)$, $Y\sim Beta(2, 2)$ and $Z\sim Uniform[0, 3]$. Find $P(X + Y > Z)$.</li>
					<span class="fragment">
						<li><strong>Proof:</strong>
							To find $ P(X + Y > Z) $ where $ X \sim N(1, 1) $, $ Y \sim \text{Beta}(2, 2) $, and
							$ Z \sim \text{Uniform}[0, 3] $, we assume independence among the variables.

						</li>
					</span>
			</section>
			<section data-background-color="#002b36" data-markdown>
				<textarea data-template>
					1. **Distributions and Independence**:
   - $ X $ is normally distributed with mean 1 and variance 1.
   - $ Y $ is beta distributed with parameters (2, 2) and is symmetric around 0.5.
   - $ Z $ is uniformly distributed over [0, 3] and is symmetric around 1.5.

				</textarea>
			</section>

			<section data-background-color="#002b36" data-markdown>
				<textarea data-template>
2. **Symmetry Argument**:
	- $ X + Y $ is symmetric around 1.5 because $ X $ is symmetric around 1 and $ Y $ is symmetric around 0.5.
	
	- $ Z $ is symmetric around 1.5.
   - The difference $ (X + Y) - Z $ is symmetric around 0 because both $ X + Y $ and $ Z $ are symmetric around 1.5.

				</textarea>
			</section>

			<section data-background-color="#002b36" data-markdown>
				<textarea data-template>
3. **Probability Calculation**:
   - Since $ (X + Y) - Z $ is symmetric around 0, the probability that $ (X + Y) - Z $ is positive is 0.5.

Thus, the probability $ P(X + Y > Z) $ is $\frac{1}{2}$.

* The model took 808 seconds, first trying many kinds of tricks to break up and integrate. 
* A little after 10 minutes it spotted the **symmetry**
* It then spent some time trying to reconcile the answer half with its previous answer of about 0.58
</textarea>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>The model then decided "The mistake in the previous calculation was a result of incorrect
						integration steps, likely due to <em>human error</em> during the complex integration by parts
						and substitutions."</li>
					<li class="fragment">However, DeepSeek-R1 struggled with the Analysis problem mentioned earlier.
					</li>
					<li class="fragment">The proof it gave split into cases and analysed these, but missed or messed up
						cases.</li>
					<li class="fragment">After 3-4 rounds of pointing out errors, it got to the proof.</li>
				</ul>
			</section>



			<section data-background-color="#002b36">
				<h4>Research Help: Gemini-2-flash-thinking</h4>
				<ul>
					<li class="fragment"><strong>Query</strong> (approx.) A research student is working on Poisson
						boundaries of groups with negative curvature. Suggest background material, useful results, and
						specific problems to work on.</li>
					<li class="fragment">Gave very useful answers, including constructions and theorems I did not know
						about.
					</li>
					<li class="fragment">Could further explain when asked about these.</li>
					<li class="fragment">Also helpful with more specific questions, other areas etc and got helpful (but
						not always accurate) answers.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Gemini-2.5-pro</h4>

				<ul>
					<li class="fragment">The analysis problem was answered cleanly by Gemini-2.5-pro.</li>
					<li class="fragment">It also solved the other questions asked.</li>
					<li class="fragment">With some back-and-forth, it helped me in research, giving a counterexample to
						a conjecture I made.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Complementing AI?</h4>
				<ul>
					<li class="fragment">We would like to <em>automatically</em> check the output of the language models
						and provide feedback.</li>
					<li class="fragment">We would also want an automatic workflow, involving decomposing problems,
						trying different things, accumulating knowledge etc.
					</li>
					<li class="fragment">Such systems should be integrated into human workflows.</li>
					<li class="fragment">So called <em>Formalization</em> and <em>Autoformalization</em> can facilitate
						this.</li>
			</section>



			<section data-transition="zoom" data-background="hills.png">
				<h2 style="color:#fffdd0">Artificial Intelligence</h2>
				<h4 style="color:#fffdd0">and</h4>
				<h2 style="color:#fffdd0">Mathematics</h2>

			</section>


			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>Consider a scenario in the future where a fully autonomous AI system can prove theorems,
						formalize them to ensure correctness, and write a paper with these theorems:
						<ul>
							<li class="fragment">At a level at least as good as 90% of the work of 90% of
								mathematicians.</li>
							<li class="fragment">On an industrial scale.
							</li>
						</ul>
					</li>
					<li class="fragment">Or tools are available to let people with limited mathematical skills
						produce
						such mathematics.</li>
					<li class="fragment">We can try to react defensively: try to detect and ban AI or focus
						mathematics on prestige (a Veblen good) rather than as knowledge (a public good).</li>
				</ul>
			</section>

			<section data-background-color="#002b36" data-transition="concave">
				<ul>
					<li>There are strong motives for the AI community to work on the
						mathematical capabilities of AI: as <strong>measurable goals/tests for reasoning</strong>.
					</li>
					<li class="fragment">However, they have little motivation to work on <strong>tools</strong> that
						make
						these useful for <strong>mathematicians</strong>.</li>
					<li class="fragment">Science has benefitted enormously from AI systems: AlphaFold 2 (and 3),
						controlling nuclear fusion, weather prediction.</li>
					<li class="fragment">Mathematics has the opportunity to grow in collaboration with AI, and the
						danger of shrinking in relevance if overtaken by autonomous AI.</li>

				</ul>
			</section>
		</section>
		<section data-background-image="LFCM2025.png" data-background-size="45%"></section>

	</div>
	</div>

	<script src="js/reveal.js"></script>


	<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/concave/concave/zoom


			math: {
				mathjax: '../MathJax/MathJax.js',
				//				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
				config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
			},

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				// { src: '../highlight/highlight.pack.js', async: true, condition: function () { return !!document.querySelector('pre code'); }, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});

		//			Reveal.addEventListener( 'slidechanged', function( event ) {
		//				MathJax.Hub.Rerender();
		//			} );

	</script>

</body>

</html>