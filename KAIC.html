<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>AlphaGeometry: AI for Maths</title>

	<meta name="description" content="Samasya Debug lecture">
	<meta name="author" content="Siddhartha Gadgil">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport"
		content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">
	<link rel="stylesheet" href="kwk.css">

	<link rel="icon" href="IIScLogo.jpg">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside this container is displayed as a slide -->
		<section class="slides">
			<section data-transition="slide">
				<h3> AlphaGeometry and friends</h3>
				<h3>Artificial Intelligence</h3>
				<h4>for</h4>
				<h3>Mathematics</h3>
				<h4>Siddhartha Gadgil</h4>
				<p>Department of Mathematics</p>
				<p>Indian Institute of Science, Bangalore</p>


			</section>


			<section data-transition="zoom-in slide-out" data-background-image="campus.jpg">
				<h1>Mathematical Proofs</h1>
			</section>

			<section data-background-color="teal">
				<h4> A proof: Sum of Angles in a Triangle</h4>
				<ul>
					<li class="fragment"> Consider a triangle $ABC$.</li>
					<img class="fragment" src="angle_sum.png" height="250" />
					<li class="fragment"> Draw a line through $A$ parallel to $BC$.</li>
					<li class="fragment"> Use equality of interior opposite angles.</li>
					<li class="fragment"> Use angles on a line add up to 180 degrees.</li>

				</ul>
			</section>
			<section data-background-color="teal">
				<h4>Another proof: infinitely many primes</h4>
				<ul>
					<li class="fragment"> For any $n$, we show that there exists a prime $p$ such that $p > n$.</li>
					<li class="fragment"> Consider the number $n!+1$, where $n! = 1 \times 2 \times \dots \times n$.
					</li>
					<li class="fragment"> Let $p$ be the smallest prime factor of $n!+1$.
					</li>
					<li class="fragment"> If $p \leq n$ then $p | n!$ so $p \nmid n!+1$.</li>
					<li class="fragment"> Hence $p$ is a prime greater than $n$.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">Components of Proving (Thinking)</h4>
				<ul>
					<li class="fragment"><strong>Intuition:</strong> Analogies, Associations etc.
						<ul>
							<li class="fragment">
								Auxiliary Constructions.
							</li>
							<li class="fragment">
								<p>Theorems/Definitions to use.</p>
							</li>
							<li class="fragment">
								Intermediate Lemmas and statements.
							</li>
						</ul>
					</li>
					<li class="fragment">
						<p><strong>Logical/Algorithmic:</strong> Deductions, Computations.</p>
					</li>
					<li class="fragment">
						Intuition is based on <strong>tacit knowledge</strong>, which is acquired by learning.
					</li>
					<li class="fragment"><strong>Moravec paradox:</strong> Intuition needs far more computation. </li>
				</ul>

			</section>


			<section data-transition="zoom-in convex-out" data-background-image="sea.jpg">
				<h1>&nbsp;</h1>
				<h1 style="color:#fffdd0">AlphaGeometry</h1>
				<p>&nbsp;</p>
			</section>



			<section data-transition="convex" data-background="#b5533c">
				<h4>AlphaGeometry</h4>
				<ul>

					<li class="fragment">AlphaGeometry is an AI system that can solve geometry problems.
					</li>
					<li class="fragment">More precisely, it can attempt to solve geometry problems which can be represented in a certain symbolic way,
						which includes most IMO geometry problems.
					</li>
					<li class="fragment">The score of AlphaGeometry on these problems was between the average score of the <strong>Silver medallists</strong> and the average score of the <strong>Gold medallists</strong> on those problems.</li> 
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Components of AlphaGeometry</h4>
				<ul>

					<li class="fragment"><strong>Deductive Database + Algebraic Reasoning (DD+AR):</strong> a powerful and efficient deductive system for reasoning about geometry problems (extending earlier work).
					</li>
					<li class="fragment">A <strong>Language Model</strong> that suggests <em>auxiliary constructions.</em>
					</li>
					<li class="fragment">Generation of <strong>synthetic data</strong> to train the language model.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Deductive Database (+ Algebraic Reasoning)</h4>
				<ul>

					<li class="fragment">In a <strong>Deductive Database</strong>, we have (efficiently represented):
						<ul>
							<li>A finite collection of points.</li>
							<li>Relations between points, lines through them etc.; for example <code>perp A B C D</code></li>
						</ul>
					</li>
					<li class="fragment">Rules of deduction let us deduce consequences.
					</li>
					<li class="fragment">For instance if <code>AB</code> and <code>CD</code>
						are both perpendicular to <code>BC</code> then they are parallel.</li>
					<li class="fragment">Iterating deduction with all applicable rules till we have a <em>fixed point</em>
					gives all consequences that <strong>do not require</strong> adding a point.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>(Deductive Database +) Algebraic Reasoning</h4>
				<ul>

					<li class="fragment">In addition to deducing relations from other relations, AlphaGeometry deduces from algebraic relations for angles, lengths and ratios.</li>
					<li class="fragment">All the relations obtained in the setup are of the form $a + b = c + d$.
					</li>
					<li class="fragment">Hence, they can be represented by a matrix and all consequences efficiently derived using <strong>Gaussian Elimination.</strong></li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Synthetic data</h4>
				<ul>

					<li class="fragment">Millions of random configurations were generated, and all theorems were deduced.</li>
					</li>
					<li class="fragment">100 million theorems were found, for which minimal proofs were generated.</li>
					</li>
					<li class="fragment">Whenever the statement of a theorem did not involve a point but the proof did, the proof involved an <em>auxiliary construction</em> (10 million).</li>
					<li class="fragment">A language model ($\approx 150M$ params) was <em>pretrained</em> on all proofs and <em>fine-tuned</em> to predict auxiliary constructions.
					</li>
					<li class="fragment">No IMO problem was in the synthetic data.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>AlphaGeometry: Solving Problems</h4>
				<ul>

					<li class="fragment">Given a geometry problem represented symbolically, all deductions are made using DD+AR.</li>
					<li class="fragment">If the deductions do not include the final conclusion, then the language model is used to generate candidate auxiliary constructions.</li>
					</li>
					<li class="fragment">For (some of) the auxiliary constructions, all deductions are made after the construction.</li>
					<li class="fragment">This process is repeated in a <strong>beam search</strong>.</li>
				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background-image="AlphaGeomPfs.jpg" data-background-size="contain">
			</section>



			<section data-transition="zoom-in concave-out" data-background="alpha-zero.jpg">
				<h1>&nbsp;</h1>
				<h2 style="color:#fffdd0">Deep Learning</h2>
				<h4>and</h4>
				<h2 style="color:#fffdd0">Language Models</h2>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Deep learning</h4>
				<ul>
					<li class="fragment">Computers can acquire knowledge by
						<em>learning</em>.
					</li>
					<li class="fragment">The idea of computers learning goes back at least to Turing, or even regression
						in statistics.
					</li>
					<li class="fragment">
						What was needed was hardware and a system that could learn efficiently: <em>Deep neural
							networks.</em>
					</li>
					<li class="fragment">In a deep neural network the input is transformed into the output through a
						series of
						layers.</li>
					<li class="fragment">Each layer is typically a linear transformation, depending on weights, followed
						by a simple
						non-linear function.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<img height="250" src="DeepNeuralNet.png" />
				<ul>
					<li class="fragment">Starting about 2012, deep neural networks performed very well in image
						processing tasks, far
						above rule-based systems.</li>
					<li class="fragment">

						It turns out that as the image is processed by successive layers, the raw data gets modified
						to capture abstraction/meaning.
					</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="limitations-as-of-about-ten-years-ago">Limitations (≈ 10 years ago)</h4>
				<ul>
					<li class="fragment">Needed labelled data for training.</li>
					<li class="fragment">Narrow: training specific to the task.</li>
					<li class="fragment">Had difficulty with long range dependencies, in particular language.</li>
					<li class="fragment">Hard to scale up, parallelize.</li>
					<li class="fragment">Could not go beyond training data.</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="representation-learning">Representation Learning</h4>
				<ul>
					<li class="fragment">
						<p><strong>Word2Vec</strong> considered the <em>synthetic</em> problem of predicting a word given its neighbours.
						</p>
					</li>
					<li class="fragment">
						<p>This was trained on solutions of this problem that involve mapping words into space, and
							predicting
							from neighbours using the points.</p>
					</li>
					<li class="fragment">
						<p>Only the mapping of words into space was retained, so <em>geometry</em> captures
							<em>associations</em>.
						</p>
					</li>
					<li class="fragment">
						<p>Another lesson is that it is useful to solve synthetic problems: play, solving exercises.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="transfer-learning">Transfer learning</h4>
				<ul>
					<li class="fragment">A set of common layers was used for many tasks.</li>
					<li class="fragment">Some of the tasks could be synthetic (exercises).</li>
					<li class="fragment">This resulted in networks trained for one task needing much less training for a
						related task.
					</li>
				</ul>

			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="attention-is-all-you-need">“Attention is all you need”</h4>
				<ul>
					<li class="fragment">The meaning of a word depends on the context, i.e., other words surrounding it.
					</li>
					<li class="fragment">In the <strong>transformer architecture</strong>, this is captured by learning
						to which other
						words to pay attention.</li>
					<li class="fragment">This allowed parallelization and captured long-range dependencies.</li>
					<li class="fragment"> With the transformer architecture training could be scaled up enormously.</li>

				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Foundation Models</h4>
				<ul>
					<li class="fragment"> It was found that models kept improving as the number of parameters and data size grew.</li>
					<li class="fragment"> Models such as GPT-4 and the Gemini models have been <strong>pretrained</strong> on "the
						internet": by next word prediction or filling in masked words.
					</li>
					<li class="fragment"> Models, especially open-source medium-sized ones (with 7B, 13B, 30B parameters) like Llama and Mistral families, can be <strong>fine-tuned</strong> 
						for specific tasks.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<ul>
					<li>Large Language Models have remarkable abilities, roughly corresponding to our
						<em>intuition</em>.
						<ul>
							<li class="fragment">They can reason by analogy.</li>
							<li class="fragment">This includes mixing and matching concepts from different domains.</li>
							<li class="fragment">They are excellent at Natural Language.</li>
							<li class="fragment">They are familiar with a huge amount of stuff.</li>
							<li class="fragment">They can transfer knowledge from one domain to another.</li>
						</ul>
					</li>
					<li class="fragment">They are, however, <strong>terrible</strong> at at avoiding logical fallacies and judging
						correctness.</li>
				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background-image="sea.jpg">
				<h1>&nbsp;</h1>
				<p>&nbsp;</p>
			</section>



			<section data-transition="convex" data-background="#b5533c">
				<h4>Cap Set problem and FunSearch</h4>
				<ul>

					<li class="fragment">The cap set problem is to find the largest possible set of vectors in $(\mathbb{Z}/3)^n$ 
						(known as a cap set) such that no three vectors sum to zero. Geometrically, no three points of a cap set are in a line.
					</li>
					<li class="fragment">Exact results are known only up to $n=6$, and brute force search is infeasible for larger $n$.
					</li>
					<li class="fragment">The system <strong>FunSearch</strong> found a construction for $n=8$ which was a significant improvement. </li>
					<li class="fragment">FunSearch did not find the cap set directly, but found a <strong>program</strong> that computed this set.</li> 
				</ul>
			</section>


			<section data-transition="convex" data-background="#b5533c">
				<h4>FunSearch: Evolutionary search</h4>
				<ul>

					<li class="fragment">Most of the program for generating cap sets is fixed; only the function <code>priority</code> is learned.
					</li>
					<li class="fragment">The function is learned by an evolution:
						<ul>
							<li>At each time we have a population of functions, starting with a single one.</li>
							<li>We evaluate and select the best functions.</li>
							<li>These are <em>combined</em> to generate new functions.</li>
						</ul>
					</li>
					<li class="fragment">Functions are combined using a Language model (Codey from the PaLM-2 family),
						 which is given instructions and the programs in its prompt. </li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Symbolic Integration</h4>
				<ul>

					<li class="fragment">(Lample & Charton): A language model was trained to symbolically integrate a given function.
					</li>
					<li class="fragment">Three forms of data were generated and used:
						<ul>
							<li><strong>Forward Generation:</strong> Generate random functions and calculate their integrals using a computer algebra system.</li>
							<li><strong>Backward generation:</strong>Generate a random function $F$, compute its derivate $f$ and note that $\int f = F$. </li>
							<li><strong>Backward generation with integration by parts (IBP)</strong>.</li>
						</ul>
					</li>
				</ul>
			</section>


			<section data-transition="convex" data-background="#b5533c">
				<h4>Backward generation with IBP</h4>
				<ul>

					<li class="fragment">We pick random functions $F$ and $G$ and compute the derivatives $f$ and $g$.
					</li>
					<li class="fragment">If $fG$ is already in the training data, we know its integral.
					</li>
					<li class="fragment">We can compute its integral as $\int Fg = FG - \int fG$.
					</li>
					<li class="fragment">This often gives short functions with longer integrals</strong>.
					</li>
					<li class="fragment">Similar approaches have been used for solving ODEs, recurrences etc.</li>
				</ul>
			</section>





			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">
				<h2 style="color:#fffdd0">Automated Theorem Provers</h2>
				<h4 style="color:#fffdd0">and</h4>

				<h2 style="color:#fffdd0">Interactive Theorem Provers</h2>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h4>SAT Problems</h4>
				<ul>
					<li class="fragment">We are given finitely many <em>boolean</em> variables $P$, $Q$, ... and finitely many
						<em>constraints</em> on them.
					</li>
					<li class="fragment">
						Constraints are logical statements built from the variables using logical operators such as
						$\neg$ (<em>not</em>), $\vee$(<em>or</em>), $\wedge$(<em>and</em>),
						$\Rightarrow$(<em>implies</em>) and $\Leftrightarrow$ (<em>equivalent</em>).
					</li>
					<li class="fragment">
						The <strong>SAT</strong> (<em>boolean satisfiability</em>) problem asks whether we can assign
						truth values to these (i.e., declare each of $P$, $Q$ , ... to be $true$ or $false$) so that all
						the constraints are satisfied.
					</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h4>SAT Solvers</h4>
				<ul>
					<li class="fragment">Many finite problems can be reduced to SAT problems; by the Cook-Levine theorem, this
						includes all problems in NP.
					</li>
					<li class="fragment">
						SAT solvers are powerful programs that can solve SAT problems, hence problems mapped to SAT.
					</li>
					<li class="fragment">SAT solvers have solved many mathematical problems, for example:</li>
					<li class="fragment"><strong>Boolean Pythagorean triples problem:</strong> Can we colour
						each
						positive integers either red or blue, so that no Pythagorean triple of integers $a$, $b$, $c$,
						satisfying $a^{2}+b^{2}=c^{2}$ is monochromatic?</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="teal">
				<h4>Resolution Theorem Provers</h4>
				<ul>
					<li class="fragment"><strong>Resolution Theorem Provers</strong> with <strong>Unification</strong>  can solve many problems in <em>first order logic</em>.
					</li>
					<li class="fragment">
						In principle, first-order logic includes all of mathematics; but, in practice, 
						the proof must depends on a small number of axioms.
					</li>
					<li class="fragment"> Robbins conjecture was a conjectural characterization of Boolean algebras in terms of
						associativity and commutativity of $\vee$ and the Robbins equation
						$\neg(\neg(a\vee b)\vee \neg(a \vee \neg b)) = a$.</li>
					<li class="fragment"> This was conjectured in the 1930s, and proved in 1996 by the
						automated theorem prover EQP.</li>
				</ul>
			</section>


			<section data-transition="convex" data-background-color="teal">
				<h4>LeanAide</h4>
				<video controls muted src="leanaide-jan24.webm"></video>
			</section>


			<section data-transition="concave" data-background-color="#002b36">
				<ul>
					<li><em>Interactive Theorem Provers</em> are software systems where proofs are
						obtained by human-machine
						collaboration.</li>
					<li class="fragment">The computer both finds (parts of) proofs and verifies
						<strong>correctness</strong>.
					</li>
					<li class="fragment">With interactive theorem provers, mathematics on a large scale can be digitied
						(<em>formalized</em>) in a modular fashion, with the automation greatly facilitating this.
					</li>
					<li class="fragment">Indeed the <em>Lean Mathematical Library</em> has most undergraduate
						mathematics in it, and some advanced topics.
					</li>
				</ul>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<ul>
					<li>As more mathematics is in such libraries, it becomes easier and more enjoyable to formalize even
						more.</li>
					<li class="fragment">Automation is also improving, which also facilitates formalization.
					</li>
					<li class="fragment"><strong>Lean 4</strong> is a <em>programming language</em> seamlessly
						integrated with an <em>interactive
							theorem prover</em>, which helps with formalization and makes it useful in new ways.
					</li>
					<li class="fragment">The main current application of Lean is to ensure a super-human
						level of correctness.
					</li>
					<li class="fragment"> One hopes this is <strong>without super-human effort</strong>.
					</li>
				</ul>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>Who guards the guards?</h4>
				<ul>
					<li class="fragment">
						A computer verified proof is only as trustworthy as the system that verified the proof.
					</li>
					<li class="fragment">
						Following the <em>de Bruijn</em> principle, proofs are <em>verified</em> by a small
						<em>trusted
							kernel</em>, which can be thoroughly checked.
					</li>
					<li class="fragment">
						For example, the Lean theorem prover has four (small) proof checkers written in
						four languages.
					</li>
					<li class="fragment">
						In a specific case, one only has to check that the <strong>definitions</strong> and the
						<strong>statement</strong> are correct, which is typically not hard especially if they use a
						library.
					</li>
				</ul>

			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>The Liquid Tensor experiment</h4>
				<ul>
					<li class="fragment">In December 2020, Peter Scholze challenged the formalization community to
						computer verify a foundational theorem of <em>Condensed
							Mathematics</em>.</li>
					<blockquote class="fragment">
						— with this theorem, the hope that the condensed formalism can be fruitfully applied to real
						functional analysis stands or falls. I think the theorem is of utmost foundational importance,
						so being 99.9% sure is not enough.
					</blockquote>

				</ul>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>This was taken up by the Lean community led by Johann Commelin, with active
						collaboration from Scholze.</li>

					<li class="fragment">
						In about six months a major step, <strong>Theorem 9.4</strong> of Scholze's notes, was proved.
					</li>
					<li class="fragment">
						Scholze said this was the part where he was worried about, and that he was now satisfied
						that
						the correctness of his result was checked.
					</li>
					<li class="fragment">
						About an year later, the origin goal, <strong>Theorem 9.1</strong> was proved in Lean.
					</li>
					<li class="fragment">Scholze said he understood aspects of his proof after the formalization which
						he hadn't earlier.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Sums of reciprocals</h4>
				<ul>

					<li class="fragment">
						In December 2021, Thomas Bloom proved that any set of natural numbers of positive upper
						density
						contains a finite subset whose reciprocals add up to $1$, answering a question of Erdős and
						Graham.
					</li>
					<li class="fragment">
						While his paper was still under review, Thomas Bloom
						learnt Lean, and,
						along with Bhavik Mehta, formalized this theorem in Lean.

					</li>

				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Kaplansky's Unit Conjecture</h4>
				<ul>

					<li class="fragment">
						About an year ago, Giles Gardam disproved the long-standing <strong>Kaplansky's Unit
							Conjecture</strong>, which asserted that group rings of torsion-free groups over fields
						do
						not have non-trivial units.
					</li>
					<li class="fragment">
						Gardam used a SAT solver to find such non-trivial units for a specific group and the field
						$\mathbb{F}_2$.
					</li>
					<li class="fragment">
						Anand Rao Tadipatri and I formalized the proof in Lean 4 using its integration of programs and
						proofs.
					</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Draft, Sketch and Prove</h4>
				<ul>
					<li class="fragment">A Large Language Model is used to generate an informal sketch of a proof.</li>
					<li class="fragment">This is translated into a series of steps so the problem is broken into
						sub-problems.</li>
					<li class="fragment">Automation is then used to complete proofs.</li>

				</ul>
			</section>


			<section data-background-color="#002b36">
				<ul>
					<li>As these examples show, the effort for formalization of mathematics is rapidly decreasing.</li>
					<li class="fragment">Mathematics in Lean is much better structured than the human literature.</li>
					<li class="fragment">Lean also has powerful automation, which helps formalization.</li>
					<li class="fragment">All this paves the way (one hopes) for facilitating mathematical discovery.
					</li>

				</ul>
			</section>


			<section data-transition="zoom" data-background="hills.png">
				<h1 style="color:#fffdd0">AI/ML + ITP</h1> 
				<h3 >for</h3> 
				<h1 style="color:#fffdd0"> Mathematics?</h1>
			</section>


			<section>
				<p>Mathematics advances in a series of steps, each of which can come from (among other things):
				<ul>
					<li class="fragment">By analogy from previous mathematics.</li>
					<li class="fragment">Combining ideas from different places.</li>
					<li class="fragment">By learning from examples.</li>
					<li class="fragment">Abstraction and generalization.</li>
					<li class="fragment">Cleverness (a la Olympiad problems).</li>
					<li class="fragment">Various heuristics such as "epsilon of room".</li>
					<li class="fragment">Refactoring clever ideas to understand.</li>
					<li class="fragment">Trying a lot of stuff and retaining what works.</li>
					<li class="fragment"> Understanding why something fails after
						making progress and modifying the approach.</li>

				</ul>
				</p>
			</section>
			<section>
				<ul>
					<li>A great piece of mathematics involves a lot of such steps, building on different pieces of
						mathematics and using different strengths in different parts.</li>
					<li class="fragment">If each step can be accomplished by AI then putting them together may just be engineering.</li> 
					<li class="fragment">Till recently the missing capability was reasoning by analogy (including generalization - which is
						analogy with an example/special case).</li>
				</ul>
			</section>
			<section>
				<ul>
				<li>While aesthetic appeal has a role in mathematics it is not necessary for guiding mathematics (at
					least in areas I know).</li>
				<li class="fragment"> At any time there are clear ultimate goals, specific test cases which are beyond the limits of
					present techniques, explicit open problems, newly developed techniques that can be extended, and
					various other guides to what is desirable worthwhile progress.</li>
				<li class="fragment"> We can work towards making AI systems capable of these, at the least making them very useful collaborators.</li>
				</ul>
			</section>
			<section>
				<h4>Collaborative uses of Deep learning</h4>
				<ul>
					<li class="fragment"> <strong>Symbolic regression:</strong> In particular the work of Francois Charton. </li>
					<li class="fragment"> <strong>Counterexamples:</strong> in graph theory due to Adam Wagner. </li>
					<li class="fragment"> <strong>Conjectures by prediction:</strong> Geordie Williamson & co., Marc Lackenby & co. </li>
					<li class="fragment"> <strong>Autoformalization</strong>. </li>
				</ul>
			</section>


			<section data-background-image="leanaide-mathlib4.gif">

			</section>


	</div>
	</div>

	<script src="js/reveal.js"></script>


	<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/concave/concave/zoom


			math: {
				mathjax: '../MathJax/MathJax.js',
				//				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
				config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
			},

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				// { src: '../highlight/highlight.pack.js', async: true, condition: function () { return !!document.querySelector('pre code'); }, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});

//			Reveal.addEventListener( 'slidechanged', function( event ) {
//				MathJax.Hub.Rerender();
//			} );

	</script>

</body>

</html>