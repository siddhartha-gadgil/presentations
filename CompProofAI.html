<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Computer Proofs and Artificial Intelligence</title>

	<meta name="description" content="Samasya Debug lecture">
	<meta name="author" content="Siddhartha Gadgil">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport"
		content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/black.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">
	<link rel="stylesheet" href="kwk.css">

	<link rel="icon" href="IIScLogo.jpg">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

	<div class="reveal">

		<!-- Any section element inside this container is displayed as a slide -->
		<section class="slides">
			<section data-transition="slide">
				<h3> Computer Proofs and</h3>
				<h3>Artificial Intelligence</h3>
				<h4>for</h4>
				<h3>Mathematics</h3>
				<h4>Siddhartha Gadgil</h4>
				<p>Department of Mathematics</p>
				<p>Indian Institute of Science, Bangalore</p>


			</section>


			<section>
				<ul>
					<li> Generative AI has become a popular sensation. </li>
					<li class="fragment"> In particular Large Language Models can generate code, database queries etc.
					</li>
					<li class="fragment">Much less well known are the major advances in computer proofs, both within
						mathematics and in industry.</li>
					<li class="fragment"> Computer proofs complement very well generative AI. </li>
					<li class="fragment"> I will describe computer proofs, generative AI and how they work together.
					</li>
					<li class="fragment"> I hope many lessons from this are useful for beyond users of computer proofs.
					</li>
				</ul>
			</section>

			<section data-transition="zoom-in slide-out" data-background-image="campus.jpg">
				<h1>(Mathematical) Proofs</h1>
			</section>

			<section data-background-color="teal">
				<h4> A proof: Sum of Angles in a Triangle</h4>
				<ul>
					<li class="fragment"> Consider a triangle $ABC$.</li>
					<img class="fragment" src="angle_sum.png" height="250" />
					<li class="fragment"> Draw a line through $A$ parallel to $BC$.</li>
					<li class="fragment"> Use equality of interior opposite angles.</li>
					<li class="fragment"> Use angles on a line add up to 180 degrees.</li>

				</ul>
			</section>
			<section data-background-color="teal">
				<h4>Another proof: infinitely many primes</h4>
				<ul>
					<li class="fragment"> For any $n$, we show that there exists a prime $p$ such that $p > n$.</li>
					<li class="fragment"> Consider the number $n!+1$, where $n! = 1 \times 2 \times \dots \times n$.
					</li>
					<li class="fragment"> Let $p$ be the smallest prime factor of $n!+1$.
					</li>
					<li class="fragment"> If $p \leq n$ then $p | n!$ so $p \nmid n!+1$.</li>
					<li class="fragment"> Hence $p$ is a prime greater than $n$.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4 id="what-went-into-proofs">What is needed for proving</h4>
				<ul>
					<li class="fragment"><strong>Intuition, Creativity:</strong> Analogies, Associations etc.
						<ul>
							<li class="fragment">
								<p>Auxiliary Constructions:</p>
								<ul>
									<li class="fragment">the parallel line</li>
									<li class="fragment">the term $n! + 1$.</li>
								</ul>
							</li>
							<li class="fragment">
								<p>Theorems/Definitions to use.</p>
							</li>
						</ul>
					</li>
					<li class="fragment">
						<p><strong>Technical Work:</strong> Deductions and Computations.</p>
					</li>
					<li class="fragment">
						<p><strong>Judgement of correctness</strong>.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal">
				<h4>Generative AI and Proofs</h4>
				<ul>
					<li> A model such as GPT-4 is:
						<ul>
							<li class="fragment"> <strong>Good</strong> at intuitive tasks.
							</li>
							<li class="fragment"> <strong>Weak</strong> at technical tasks: but can <em>delegate</em>.
							</li>
							<li class="fragment"> <strong>Terrible</strong> at judging whether reasoning is correct.
							</li>

						</ul>
					</li>
				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Formal Proofs</h4>
				<ul>
					<li class="fragment"> A <strong>formal proof</strong> is a sequence of deductions
						from <em>assumptions</em> and <em>axioms</em> using precise rules.
					</li>
					<li class="fragment">A formal proof can be easily checked by a computer or a human.</li>
					</li>
					<li class="fragment"> Writing (and checking) a fully formal proof is difficult and tedious except in
						the simplest cases.</li>
					<li class="fragment">With advances in Logic/PL, specifically <em>Dependent
							Type Theory</em>, formal proofs are shorter, more modular and more transparent.</li>
					</li>
					<li class="fragment"> Computers can assist in both verification and finding formal proofs.</li>
				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Informal proofs</h4>
				<ul>
					<li class="fragment"> In practice mathematical proofs are <em>informal</em>; consisting of a
						sequence of assertions and hints as to why they are true.</li>
					<li class="fragment">A reader has to judge whether the proof is correct.</li>
					<li class="fragment">A correct proof must: be <strong>precise</strong>; have n <strong>false statements</strong>; 
						use <strong>hypothesis</strong> without which the result is false; etc.</li>  
					</li>
					<li class="fragment">In practice there are many proofs, including those published in top journals,
						that are <strong>wrong.</strong></li>
					<li class="fragment">Proofs are a defining feature of mathematics;  <span class="fragment">but a wrong proof never killed anyone.</span></li>

				</ul>
			</section>

			<section data-transition="zoom-in convex-out" data-background-image="sea.jpg">
				<h1>&nbsp;</h1>
				<h1 style="color:#fffdd0">Formal methods</h1>
				<h2>Proofs elsewhere</h2>
				<p>&nbsp;</p>
			</section>



			<section data-transition="convex" data-background="#b5533c">
				<h4>Formal methods</h4>
				<ul>

					<li class="fragment">We <strong>specify</strong> (describe) software, hardware, networking
						protocols, cryptographic protocols, even tax codes in precise mathematical terms.
					</li>
					<li class="fragment">We give <strong>mathematical proofs</strong> of correct behaviour, which are
						<strong>computer
							verified</strong>.
					</li>
					<li class="fragment">This gives a much greater certainty of correctness.</li>
					<li class="fragment">However, proofs are much harder than tests.</li>
					<li class="fragment">As technology for formal proving becomes better, the much greater assurance of
						correctness will encouraging proving over testing in more situations.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>TimSort</h4>
				<ul>

					<li class="fragment">Tim Peters developed the Timsort hybrid sorting algorithm in 2002 &ndash; a
						clever combination of ideas from merge sort and insertion sort designed to perform well on real
						world data.
					</li>
					<li class="fragment">This was initially developed for Python, but was ported to Java and was used as
						the default sorting algorithm for Android SDK, Sun’s JDK and OpenJDK.
					</li>
					<li class="fragment">This means that the number of computers, cloud services and mobile phones that
						use TimSort for sorting is well into the billions.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background="#b5533c">
				<h4>Proving TimSort?</h4>
				<ul>

					<li class="fragment">Having successfully computer verified Counting and Radix sort implementations
						in Java, the European <em>envisage</em> group turned to verifying TimSort.
					</li>
					<li class="fragment">They were not able to verify correctness.
					</li>
					<li class="fragment">A closer analysis showed that this was because the algorithm was wrong.</li>
					<li class="fragment">The researchers could find, and suggest a fix for, the bug.</li>
				</ul>
			</section>

			<section data-transition="convex" data-background-color="#b5533c">
				<p>Byron Cook (AWS): </p>
				<blockquote class="fragment">
					The storage team, for example, is able to be much more agile and be much more aggressive in the
					programs that they write because of the formal methods. They're able to write code that otherwise
					they might not want to deploy because they wouldn't be as confident about it, and they're deploying
					four times as fast.
				</blockquote>
			</section>





			<section data-transition="convex" data-background-color="#b5533c">
				<h4>When do we need <em>completely correct always</em>?</h4>
				<ul>

					<li class="fragment"><strong>Safety critical systems:</strong> e.g., Paris driverless metro.
					</li>
					<li class="fragment"><strong>Costly to fix:</strong> e.g., Intel chips.
					</li>
					<li class="fragment"><strong>Core system functon:</strong> e.g., TimSort.</li>
					<li class="fragment"><strong>System level:</strong> a bug is a <em>vulnerability</em> e.g., AWS.
					</li>
				</ul>

			</section>





			<section data-transition="zoom-in concave-out" data-background-image="campus.jpg">
				<h1 style="color:#fffdd0">Interactive Theorem Provers</h1>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<ul>
					<li><em>Interactive Theorem Provers</em> are software systems where proofs are
						obtained by human-machine
						collaboration.</li>
					<li class="fragment">The computer both finds (parts of) proofs and verifies
						<strong>correctness</strong>.
					</li>
					<li class="fragment">With interactive theorem provers, mathematics on a large scale can be digitied
						(<em>formalized</em>) in a modular fashion, with the automation greatly facilitating this.
					</li>
					<li class="fragment">Indeed the <em>Lean Mathematical Library</em> has most undergraduate
						mathematics in it, and some advanced topics.
					</li>
				</ul>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<ul>
					<li>As more mathematics is in such libraries, it becomes easier and more enjoyable to formalize even
						more.</li>
					<li class="fragment">Automation is also improving, which also facilitates formalization.
					</li>
					<li class="fragment"><strong>Lean 4</strong> is a <em>programming language</em> seamlessly
						integrated with an <em>interactive
							theorem prover</em>, which helps with formalization and makes it useful in new ways.
					</li>
					<li class="fragment">The main current application of Lean is to ensure a super-human
						level of correctness.
					</li>
					<li class="fragment"> One hopes this is without super-human
						effort.
					</li>
				</ul>
			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>Who guards the guards?</h4>
				<ul>
					<li class="fragment">
						A computer verified proof is only as trustworthy as the system that verified the proof.
					</li>
					<li class="fragment">
						Following the <em>de Bruijn</em> principle, proofs are <em>verified</em> by a small
						<em>trusted
							kernel</em>, which can be thoroughly checked.
					</li>
					<li class="fragment">
						For example, the Lean theorem prover has four (small) proof checkers written in
						four languages.
					</li>
					<li class="fragment">
						In a specific case, one only has to check that the <strong>definitions</strong> and the
						<strong>statement</strong> are correct, which is typically not hard especially if they use a
						library.
					</li>
				</ul>

			</section>

			<section data-transition="concave" data-background-color="#002b36">
				<h4>The Liquid Tensor experiment</h4>
				<ul>
					<li class="fragment">In December 2020, Peter Scholze challenged the formalization community to
						computer verify a foundational theorem of <em>Condensed
							Mathematics</em>.</li>
					<blockquote class="fragment">
						— with this theorem, the hope that the condensed formalism can be fruitfully applied to real
						functional analysis stands or falls. I think the theorem is of utmost foundational importance,
						so being 99.9% sure is not enough.
					</blockquote>

				</ul>
			</section>
			<section data-background-color="#002b36">
				<ul>
					<li>This was taken up by the Lean community led by Johann Commelin, with active
						collaboration from Scholze.</li>

					<li class="fragment">
						In about six months a major step, <strong>Theorem 9.4</strong> of Scholze's notes, was proved.
					</li>
					<li class="fragment">
						Scholze said this was the part where he was worried about, and that he was now satisfied
						that
						the correctness of his result was checked.
					</li>
					<li class="fragment">
						About an year later, the origin goal, <strong>Theorem 9.1</strong> was proved in Lean.
					</li>
					<li class="fragment">Scholze said he understood aspects of his proof after the formalization which
						he hadn't earlier.</li>
				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Sums of reciprocals</h4>
				<ul>

					<li class="fragment">
						In December 2021, Thomas Bloom proved that any set of natural numbers of positive upper
						density
						contains a finite subset whose reciprocals add up to $1$, answering a question of Erdős and
						Graham.
					</li>
					<li class="fragment">
						While his paper was still under review, Thomas Bloom
						learnt Lean, and,
						along with Bhavik Mehta, formalized this theorem in Lean.

					</li>

				</ul>
			</section>

			<section data-background-color="#002b36">
				<h4>Kaplansky's Unit Conjecture</h4>
				<ul>

					<li class="fragment">
						About an year ago, Giles Gardam disproved the long-standing <strong>Kaplansky's Unit
							Conjecture</strong>, which asserted that group rings of torsion-free groups over fields
						do
						not have non-trivial units.
					</li>
					<li class="fragment">
						Gardam used a SAT solver to find such non-trivial units for a specific group and the field
						$\mathbb{F}_2$.
					</li>
					<li class="fragment">
						Anand Rao Tadipatri and I formalized the proof in Lean 4 using its integration of programs and
						proofs.
					</li>

				</ul>
			</section>

			<section data-background-color="#002b36">
				<ul>
					<li>As these examples show, the effort for formalization of mathematics is rapidly decreasing.</li>
					<li class="fragment">Mathematics in Lean is much better structured than the human literature.</li>
					<li class="fragment">Lean also has powerful automation, which helps formalization.</li>
					<li class="fragment">All this paves the way (one hopes) for facilitating mathematical discovery.
					</li>

				</ul>
			</section>

			<section data-transition="zoom-in concave-out" data-background="alpha-zero.jpg">
				<h1>&nbsp;</h1>
				<h2 style="color:#fffdd0">Stages of</h2>
				<h1 style="color:#fffdd0">Artificial Intelligence</h1>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="common-sense">The common sense barrier</h4>
				<ul>
					<li class="fragment">We know a lot of things, on which we base <em>common sense</em> (according to
						Marvin Minsky).</li>
					<li class="fragment">Further, these pieces of knowledge are organized in a way that they can be
						combined.</li>
					<li class="fragment"><strong>Expert Systems</strong> have a lot of knowledge encoded as structured
						data, functions,
						algorithms.</li>
					<li class="fragment">These are purely System 2, i.e., logical/algorithmic/rule-based systems.</li>
					<li class="fragment">Besides knowledge and fast algorithms, we also need enough raw power.</li>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Kasparov vs Deep Blue</h4>
				<ul>
					<li class="fragment">In 1997, a computer <strong>Deep Blue</strong> defeated the Chess world
						champion Gary Kasparov.
					</li>
					<li class="fragment"> Deep Blue was an <em>Expert system</em>, based on elaborate rules for judging
						strengths of positions.
					</li>
					<li class="fragment">However, Deep Blue was very limited in certain
						<strong>capabilities</strong>.
					</li>
					<li class="fragment"> Kasparov was far better at evaluating strengths of positions than Deep Blue,
						and in knowing what moves are worth considering (System 1). </li>
					<li class="fragment"> Deep Blue compensated for this by being able to consider far more move
						sequences.</li>
				</ul>

			</section>


			<section data-background-color="teal" data-transition="concave">
				<h4 id="tacit-knowledge">The tacit knowledge barrier</h4>
				<ul>
					<li class="fragment"><strong>Tacit knowledge</strong> is the kind of knowledge that is difficult to
						transfer to another person by
						means of writing it down or verbalizing it.</li>
					<li class="fragment">Examples: riding a bicycle, speaking a language; evaluating positions in chess.
					</li>
					<li class="fragment">Tacit knowledge is much more important in the chinese game Go compared to
						Chess.</li>
					<li class="fragment">Further, in Go the number of legal moves is much larger so brute force is less
						effective.</li>
					<li class="fragment">As a result, DeepBlue style expert systems are hopeless at Go.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Deep learning</h4>
				<ul>
					<li class="fragment">Computers can acquire knowledge by
						<em>learning</em>.
					</li>
					<li class="fragment">The idea of computers learning goes back at least to Turing, or even regression
						in statistics.
					</li>
					<li class="fragment">
						What was needed was hardware and a system that could learn efficiently: <em>Deep neural
							networks.</em>
					</li>
					<li class="fragment">In a deep neural network the input is transformed into the output through a
						series of
						layers.</li>
					<li class="fragment">Each layer is typically a linear transformation, depending on weights, followed
						by a simple
						non-linear function.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<img height="250" src="DeepNeuralNet.png" />
				<ul>
					<li class="fragment">Starting about 2012, deep neural networks performed very well in image
						processing tasks, far
						above rule-based systems.</li>
					<li class="fragment">

						It turns out that as the image is processed by successive layers, the raw data gets modified
						to capture abstraction/meaning.
					</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="limitations-as-of-about-ten-years-ago">Limitations (≈ 10 years ago)</h4>
				<ul>
					<li class="fragment">Needed labelled data for training.</li>
					<li class="fragment">Narrow: training specific to the task.</li>
					<li class="fragment">Had difficulty with long range dependencies, in particular language.</li>
					<li class="fragment">Hard to scale up, parallelize.</li>
					<li class="fragment">Could not go beyond training data.</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4 id="representation-learning">Representation Learning</h4>
				<ul>
					<li class="fragment">
						<p><strong>Word2Vec</strong> considered the problem of predicting a word given its neighbours.
						</p>
					</li>
					<li class="fragment">
						<p>This was trained on solutions of this problem that involve mapping words into space, and
							predicting
							from neighbours using the points.</p>
					</li>
					<li class="fragment">
						<p>Only the mapping of words into space was retained, so <em>geometry</em> captures
							<em>associations</em>.
						</p>
					</li>
					<li class="fragment">
						<p>Another lesson is that it is useful to solve synthetic problems: play, solving exercises.</p>
					</li>
				</ul>

			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="transfer-learning">Transfer learning</h4>
				<ul>
					<li class="fragment">A set of common layers was used for many tasks.</li>
					<li class="fragment">Some of the tasks could be synthetic (exercises).</li>
					<li class="fragment">This resulted in networks trained for one task needing much less training for a
						related task.
					</li>
				</ul>

			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4>AlphaGo vs Lee Sedol</h4>
				<ul>
					<li class="fragment"> In March 2016, a Go playing system AlphaGo defeated 18-time world
						champion Lee Sedol.</li>
					<li class="fragment"> In January 2017, AlphaGo defeated the world number one Ke Jie
						comprehensively.
					</li>
					<li class="fragment"> The strategy of AlphaGo was based on
						<ul>
							<li> A <em>policy</em> function - what moves to consider. </li>
							<li> A <em>value</em> function - how good a position is. </li>
						</ul>
					</li>
					<li class="fragment"> Given these, Go was played using a search through possible moves suggested by
						policy.
					</li>
				</ul>
			</section>
			<section data-background-color="teal" data-transition="concave">
				<h4>AlphaGo and Learning</h4>
				<ul>
					<li class="fragment"> The policy and value
						functions of AlphaGo are deep neural networks
						that were trained.
					</li>
					<li class="fragment"> The policy network was initially trained by learning to predict the next
						move from games of expert players (<em>behaviour cloning</em>).</li>
					<li class="fragment"> The value network was trained by AlphaGo playing against
						itself: <strong>Reinforcement Learning</strong>.
					</li>
					<li class="fragment"> AlphaGo considered fewer sequences of moves than Deep Blue.</li>
					<li class="fragment"> AlphaGo came up with unexpected moves.</li>
				</ul>
			</section>


			<section data-background-color="teal" data-transition="concave">
				<h4>AlphaGo Zero and Alpha Zero</h4>
				<ul>
					<li class="fragment">AlphaGo was succeeded (and defeated) by <em>AlphaGo Zero</em>, which learnt
						purely by
						self play.</li>
					<li class="fragment">Its successor, <em>AlphaZero</em>, could master a variety of similar games
						starting with
						just the rules.</li>
					<li class="fragment">AlphaZero took just 4 hours to become the strongest chess player on the
						planet
						(beating a traditional chess program, Stockfish).</li>
					<li class="fragment">AlphaZero &ldquo;had a dynamic, open style&rdquo;, and &ldquo;prioritizes
						piece
						activity over material, preferring positions that looked risky and aggressive.&rdquo;</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4 id="attention-is-all-you-need">“Attention is all you need”</h4>
				<ul>
					<li class="fragment">The meaning of a word depends on the context, i.e., other words surrounding it.
					</li>
					<li class="fragment">In the <strong>transformer architecture</strong>, this is captured by learning
						to which other
						words to pay attention.</li>
					<li class="fragment">This allowed parallelization and captured long-range dependencies.</li>
					<li class="fragment">Transformers are used in <strong>AlphaFold 2</strong> which predicts protein
						structures on par with experiments.</li>
				</ul>
			</section>

			<section data-background-color="teal" data-transition="concave">
				<h4>Foundation Models</h4>
				<ul>
					<li class="fragment"> With the transformer architecture training could be scaled up enormously.</li>
					<li class="fragment"> It was found that models kept improving as the number of parameters grew (when
						trained on huge datasets).</li>
					<li class="fragment"> Huge models such as GPT-3 and PaLM have been trained on essentially all of the
						internet, either by trying to predict the next word or by trying to fill in masked out parts.
					</li>
					<li class="fragment"> Applications: Dall-E 2, ChatGPT etc., have become sensations.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<ul>
					<li>Large Language Models have remarkable abilities, roughly corresponding to our
						<em>intuition</em>.
						<ul>
							<li class="fragment">They can reason by analogy.</li>
							<li class="fragment">This includes mixing and matching concepts from different domains.</li>
							<li class="fragment">They are excellent at Natural Language.</li>
							<li class="fragment">They are familiar with a huge amount of stuff.</li>
							<li class="fragment">They can transfer knowledge from one domain to another.</li>
						</ul>
					</li>
				</ul>
			</section>

			<section data-background-color="teal">
				<ul>
					<li>Large Language Models (generative AI) are weak at technical work such as calculations.</li>
					<li class="fragment">But other software is good at calculations: so one can get a language model to
						output a Python program that computes the answer.</li>
					<li class="fragment">They are hopeless at judging whether their output is correct.</li>
					<li class="fragment">But they can correct errors based on error messages.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Minerva</h4>
				<ul>
					<li class="fragment">Minerva is a Large Language Model (LLM) capable of solving mathematical and
						scientific questions using step-by-step reasoning.</li>
					<li class="fragment">It can answer reasonably complex problems, including some JEE Mains questions.
					</li>
					<li class="fragment">Minerva uses various techniques such as:
						<ul>
							<li>few-shot prompting,</li>
							<li>chain of thought or scratchpad prompting,</li>
							<li>majority voting.</li>
						</ul>
					</li>
					<li class="fragment">However there is no automatic way to check correctness, hence to further train.
					</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4>GPT-f, PACT, HTPS</h4>
				<ul>
					<li class="fragment">Proofs can be given in a system such as the Lean Theorem Prover as a sequence
						of <em>tactics</em>.</li>
					<li class="fragment">Language Language Models are used to predict useful tactics.</li>
					<li class="fragment">The system searches for a proof using these tactics.</li>
					<li class="fragment">These systems have managed to solve some IMO problems.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Draft, Sketch and Prove</h4>
				<ul>
					<li class="fragment">A Large Language Model is used to generate an informal sketch of a proof.</li>
					<li class="fragment">This is translated into a series of steps so the problem is broken into
						sub-problems.</li>
					<li class="fragment">Automation is then used to complete proofs.</li>

				</ul>
			</section>

			<section data-background-color="teal">
				<h4>Baldung</h4>
				<ul>
					<li class="fragment">This system is based on two Large Language Models.</li>
					<li class="fragment">The first model generates a candidate proof.</li>
					<li class="fragment">The candidate proof is run through the interactive theorem prover. The second
						model suggests repairs based on these errors.</li>
				</ul>
			</section>


			<section>
				<h3>Mathematical discovery</h3>
				<p>Mathematics advances in a series of steps, each of which can come from (among other things):
				<ul>
					<li>By analogy from previous mathematics.</li>
					<li>Combining ideas from different places.</li>
					<li>By learning from examples: for instance deeply understanding an example of a construction of
						Jorgensen (of a hyperbolic structure) was a key part of Thurston's geometrrization.</li>
					<li>Abstraction and generalization.</li>
					<li>Cleverness (a la Olympiad problems).</li>

				</ul>
				</p>
			</section>
			<section>
				<ul>
					<li>Various heuristics such as "epsilon of room".</li>
					<li>Refactoring clever ideas to understand (to quote Dennis Sullivan: "why be clever when we can be
						intelligent").</li>
					<li>Trying a lot of stuff and retaining what works. Also understanding why something fails after
						making progress and modifying.</li>
				</ul>
			</section>
			<section>
				<ul>
					<li>A great piece of mathematics involves a lot of such steps, building on different pieces of
						mathematics and using different strengths in different parts.</li>
					<li>If each step can be accomplished by AI then putting them together may just be engineering. Till
						recently the missing capability was reasoning by analogy (including generalization - which is
						analogy with an example/special case).</li>
				</ul>
			</section>
			<section>
				<ul>
				<li>While aesthetic appeal has a role in mathematics it is not necessary for guiding mathematics (at
					least in areas I know).</li>
				<li> At any time there are clear ultimate goals, specific test cases which are beyond the limits of
					present techniques, explicit open problems, newly developed techniques that can be extended, and
					various other guides to what is desirable worthwhile progress.</li>
				<li> We can work towards making AI systems capable of these, at the least making them very useful collaborators.</li>
				</ul>
			</section>


			<section data-background-image="leanaide-mathlib4.gif">

			</section>


	</div>
	</div>

	<script src="js/reveal.js"></script>


	<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,

			transition: 'slide', // none/fade/slide/concave/concave/zoom


			math: {
				mathjax: '../MathJax/MathJax.js',
				//				mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
				config: 'TeX-AMS_HTML-full' // See http://docs.mathjax.org/en/latest/config-files.html
			},

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function () { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: 'plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				// { src: '../highlight/highlight.pack.js', async: true, condition: function () { return !!document.querySelector('pre code'); }, callback: function () { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/highlight/highlight.js', async: true },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true },
				{ src: 'plugin/math/math.js', async: true }
			]
		});

//			Reveal.addEventListener( 'slidechanged', function( event ) {
//				MathJax.Hub.Rerender();
//			} );

	</script>

</body>

</html>